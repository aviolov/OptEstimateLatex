\chapter{Mathematical Background}
\label{ch:math_background}
\input{../OptEstimate/local_style.sty}
Here we collect a list of mathematical tools that are used in the thesis

We cite amongst others Oksendal for SDEs \cite{Oksendal2007} and Fleming and
Rishel for Optimal Control \cite{Fleming1975}.

A very readable introduction to the field of both SDEs and Optimal Control are
the online notes of Professor L. Evans \cite{Evansa,Evansb}. We have also used
Jacobs as our main tutorial on first-passage times for SDEs \cite{Jacobs}. 
 
\section{Stochastic Differential Equations}
\label{sec:SDEs}
In view of our ultimate goals, we will restrict ourselves to  stochastic
processes whose sample paths have continuous paths, i.e.\ to SDEs driven by
Brownian motion.  

We will not provide proofs for results in this section, but only state the
results with a view towards establishing the notation for the sequel.

We will assume that the reader is familiar with the following concepts:
\begin{itemize} 
  \item a probability space, $\{\O, \sAlg, P\}$ consisting of a
  probability space, a sigma-algebra and a probability measure
  \item a continuous-time stochastic process, $X_t$
  \item a filtration $\Fil(t)$, in particular the filtration generated by a
  stochastic process.
\end{itemize}

\subsection{The Wiener Process and the It\^o Integral}
The Wiener Process is the fundamental building block of the stochastic calculus,
it is often called Brownian Motion and we will denote it $\{W_t\}_{t\geq 0}$. It
satisfies:
\begin{defn}Wiener Process, $W_t$:
\begin{enumerate}
  \item $W_0 = 0$
  \item $W_t - W_s = N(0, |t-s|)$ , i.e. normally distributed increments with
  mean 0 and variance $|t-s|$)
  \item $\forall \{t_i\}_1^N, \quad \{W_{t_i} - W_{t_{i-1}} \}_2^N \sim$
  independent, i.e $W_t$ has independent increments
\end{enumerate}
\end{defn}
The fact that the finite incremental distributions suffice to specify a unique
continuous-time stochastic process is known as Kolmogorov's extension theorem.
Now, we collect a few more relevant properties of $W_t$:
\begin{enumerate}
  \item The sample paths $W_{[0,\infty)}(\o)$
are almost surely (a.s.) continuous and are in fact Holder continuous for any
exponent $\g < 1/2$
\item The sample paths $W_{[0,\infty)}(\o)$ are nowhere differentiable
\item $W_t$ is a Markov process: $\Prob[W_t \in B | \s(W_{s' \leq s })] =
\Prob[W_t \in B \,| \,W_s]$, for any Borel set, $B \subset \R$, where $\s(W_{s'
\leq s })$ is the filtration generated by the $W_t$
\end{enumerate}

We now turn to defining stochastic integrals based on the Wiener Process.
 
\begin{defn} Progressively Measurable Functions:

Let $\Fil_t$ be the filtration generated by the Wiener Process.

Let $X_t$ be a stochastic process which is $\Fil_t$-measurable $\forall t$ and
which is jointly measurable in $(t,\o)$. We call such an $X_t$
\emph{progressively measurable}
\end{defn}

\begin{defn} $\Ltwopm, \Lonepm$

We define $\Ltwopm[0,T]$ as the space of all progressively measurable
$X$ such that
\begin{equation*}
\Exp \left[ \int_{[0,T]} X_t^2 \intd{t} \right] < \infty
\end{equation*}

Similarly, we define $\Lonepm[0,T]$ as the space of all progressively measurable
$X$ such that
\begin{equation*}
\Exp \left[ \int_{[0,T]} X_t \intd{t} \right] < \infty
\end{equation*} 
\end{defn}

$\Ltwopm$ will be the class of functions for which the It\^o integral is
well-defined. 
\begin{defn} It\^o Integral:
\label{defn:ito_integral}

Let $P^n := {a = t^n_1 \ldots t^n_{m_n} = b}$ be a partition of the interval
$[a,b] \subset [0, \infty)$. Let $|P_n| = \sup_i|t_i - t_{i-1}|$. 
Take $|P_n| \rightarrow_n 0 $ and consider an $ X_t \in \Ltwopm[a,b]$,

then
\begin{equation}
\int_{[a,b]} X_t \intd{W} := \lim_{n \rightarrow \infty}  
\sum_{i=1}^{m_n} X_{t_i}\left( W(t_{i+1}) - W(t_{i})\right)
\end{equation}

\end{defn}

To be precise, our definition is actually a theorem, and the real
definition is one that uses step functions and passes to the limit.  Also we
will write the limits of integration $\int_0^T \cdot  \intd{W}$ or $\int_{[0,T]}
\cdot  \intd{W}$ interchangeably. 

Again, we state without proof a few interesting properties of $\int X \intd{W}$:
\begin{thm} It\^o Integral Properties

\begin{enumerate}
  \item $\Exp[\int_0^T X \intd{W} ] = 0$ 
  \item $\Exp[\left(\int_0^T X \intd{W}\right)^2 ] = \Exp[\int_0^T X^2
  \intd{t}]$
  \item $I(t) = \int_0^t X \intd{W} $ is a martingale 
  \item $I(t) = \int_0^t X \intd{W} $ has continuous sample paths
\end{enumerate}
\end{thm}

\subsection{It\^o SDEs and It\^o's Lemma}
\begin{defn}[It\^o SDE]

We write
\begin{equation}
dX_t = F dt + G dW
\end{equation}
on $0 \leq t \leq T$, if $X$ is a real-valued stochastic process satisfying:
\begin{equation*}
X(r) = X(s) + \int_{[s,r]} F \intd{t} + \int_{[s,r]} G \intd{W}
\quad 0\leq s \leq r \leq T
\end{equation*}
for some $F \in \Lonepm[0,T]$ and $G \in \Ltwopm[0,T]$
\end{defn}

We are now ready to present the celebrated It\^o Lemma which is the chain-rule of
stochastic calculus:

\begin{thm}[It\^o Lemma]
Suppose $X$ satisfies the stochastic differential $dX_t =
Fdt + G dW$ as above and take $v(x,t) \in C^{2,1}[ \R \times [0,T]]$.

Set $Y(t) = v(X,t)$
\\
then
$$
dY =  \left( \di_t v + \di_x v \cdot F + \di^2_x v \cdot \frac{G^2}2 \right)
\intd{t} + \left(   \di_x v\cdot G  \right)\intd{W}
$$
\end{thm} 

Finally, we are in a position to define an It\^o SDE:

\begin{defn}[It\^o SDE] Let
$F(x,t):\R\times [0,T]\ra \R,  G(x,t):\R\times [0,T]\ra \R,$ be given functions.
We say that a stochastic process $X$ satisfies:
\begin{equation}
dX =F dt + G dW
\end{equation} 
over $[0,T]$ if
\begin{enumerate}
  \item $X$ is progressively measurable wrt. $\Fil_t$
  \item $F(X,t) \in \Lonepm$
  \item $G(X,t) \in \Ltwopm$ 
  \item $X_t = X_0 + \int_0^t F(X_s, s) \intd{s} + \int_0^t G(X_s, s) \intd{W_s}$
\end{enumerate}
\end{defn} 

As an example we will discuss the Ornstein-Uhlenbeck process, which is the
basis for the models we will face later on.
%TODO: Decide on whether it is (m-x)/t or m - x/t, once and for all!
\begin{ex}[O-U Process] Let $X$ follow:
\begin{equation}
dX = \left( \frac {\m -X_t}{\tc} \right) dt + \b dW
\label{eq:OU_equation_generic}
\end{equation}
with an initial condition, $X_0$, which may be an arbitrary distribution
independent of the Wiener Process, $W$.
We can solve this as follows:
%TODO: Solve OU with this parameter set:
\begin{align*}
dX =& \left( \frac {\m -X_t}{\tc} \right)  dt + \b dW
\\
dX + \frac {X_t}{\tc} dt=&  \frac\m\tc dt + \b dW
\\
e^{t/\tc} dX + e^{t/\tc}\frac {X_t}{\tc} dt
=& e^{t/\tc}\frac\m\tc dt + \b e^{t/\tc} dW
\\
Xe^{t/\tc} - X_0 
=& \int  e^{t/\tc}\frac\m\tc dt +  \int \b e^{t/\tc} dW
\\
X_t =& e^{-t/\tc} X_0 + \m(1-e^{-t/\tc}) +  \frac{\sqrt{\tc}\b
e^{-t/\tc}}{\sqrt{2}} W(e^{2t/\tc}-1)
\end{align*}
which means that $X$ forgets its initial conditions exponentially fast and
converges to a normal random variable with mean $\m$ and variance
$\tfrac{\tc \b^2}{2}$
\end{ex}

At the end of this sub-section, we state It\^o's Lemma with multiple Wiener
processes and then for a multidimensional state.

\begin{thm}[It\^o Lemma for $dW \in  \R^{m}$] Suppose $X$ satisfies the stochastic
differential $dX_t = F dt + G dW$, where $ F \in \R, G \in \R^{1\times m}$ and
$dW = \left(dW^{(i)}\right) \in  \R^{m}$ is a vector of independent Wiener
Processes. Take $u(x,t) \in C^{2,1}[ \R \times [0,T]]$. Let $D :=\left[{\sum_k
G_{k} G_{k} } \right] \in \R$

Set $Y(t) = u(X,t)$
\\
then
$$
dY =  \left( \di_t u +  \di_{x} u \cdot F + 
  \di^2_{x} u \cdot D \right)
\intd{t} +
 \left(  \di_{x} u  \cdot  G \right) dW  
$$
\end{thm}

\begin{thm}[It\^o Lemma for $X \in R^n$]
Suppose $X$ satisfies the stochastic differential $dX_t = F dt + G dW$, where $
F \in \R^n, G \in \R^{n\times m}$ and $dW = \left(dW^{(i)}\right) \in  \R^{ 
m}$ is a vector of independent Wiener Processes. Take $u(x,t) \in C^{2,1}[ \R^n
\times [0,T]]$. Let $D_{ij} := \tfrac{1}{2} \left[{\sum_k G_{ik} G_{jk} }
\right]_{ij} \in \R^{n\times n} $

Set $Y(t) = u(X,t)$
\\
then
$$
dY =  \left( \di_t u + \sum_i \di_{x_i} u \cdot f_i + 
\sum_{i,j} \di^2_{x_i x_j} u \cdot D_{ij} \right)
\intd{t} +
 \left(  \sum_{ij} \di_{x_i} u  \cdot G_{ij} dW^{(j)} 
\right)$$
\end{thm} 

\subsection{Fokker-Planck and Kolmogorov's Backward Equations}
The Fokker-Planck equation and Kolmogorov's Backward equation describe the
forward (resp. backward)  evolution of the probability density of $X_t$

For the rest of this section we will work in $ \R^n$, i.e. $X$ will be an $n-$
dimensional vector that satisfies the stochastic differential
\begin{equation}
dX_t = F(X,t) dt + G(X,t) dW,
\label{eq:generic_Ito_SDE_Rn}
\end{equation}
$F \in \R^n, G \in \R^{n \times m}$,
$dW = (dW^{(i)}) \in  \R^{  m}$ is an $m$-dimensional Brownian motion and we
write $D_{ij} :=
\tfrac{1}{2} 
\left[{\sum_k G_{ik} G_{jk} } \right]_{ij} \in \R^{n\times n} $.

Let 
\begin{equation}
 \f(x,t| y,s) \intd{x} =  \Prob[X_t \in \intd{x} | X_s = y]
 \label{eq:transition_prob_defn} 
 \end{equation}
be the transition probability density. Then $\f$ satisfies:
\begin{equation}
\di_t \f= -\sum_i \di_{x_i} \left[ F_i(x,t) \f(x,t) \right] 
+ 
\sum_{i,j}  \di^2_{x_i x_j} \left[ D_{ij}(x,t) \f(x,t) \right]
\label{eq:generic_FokkerPlanck}
\end{equation} 
This is called the \emph{Fokker-Planck} or \emph{Forward Kolmogorov} equation.
It can also be seen as a continuity euquation or a cosevation of probability
equation. To this end define the probability current, $\p \in \R^n$ as:
$$
\phi_i =   F_i(x,t) \f(x,t) 
+ 
\sum_{j}  \di_{x_j} \left[ D_{ij}(x,t) \f(x,t)\right]
$$
then the Fokker-Planck equation, \cref{eq:generic_FokkerPlanck}, can be written as:
$$
\di_t \f= -\grad \cdot \phi
$$
which just says that the change in probability is the
difference between the flow in and the flow out. 

Initial conditions for $\f$ are given by the distribution of $X_s$, if $s=0$
then this is the initial distribution of $X$. Boundary conditions (BCs) depend
on the domain of $X$ and what happens to $X$ once it hits its boundary. If the domain
is all of $\R^n$ then we only insist that $\lim_{|x| \ra \infty} \f = 0$. If the
domain has boundaries, then there are two common scenarios which we will also
encounter in the sequel:
\begin{enumerate}  
  \item absorbing BCs
  \item reflecting BCs
\end{enumerate}
At an absorbing boundary, the particle $X$ is removed and $\f=0$ there.
In this situation, we will not have conservation of probability and the integral
of $\f$ over $X$'s domain will monotonically decrease.

At a reflecting boundary, the particle $X$ bounces back into its domain and we
will have that the probability current $\p\cdot n = 0$ where $n$ is the outward
normal at the reflecting boundary.

We now consider $\f$ as functions of $y,s$, holding $x,t$ fixed, then
\begin{equation}
-\di_s \f= \sum_i  \left[F_i(y,s) \cdot \di_{y_i}\f(|y,s) \right] 
+ 
\sum_{i,j}   \left[ D_{ij}(x,t) \di^2_{y_i y_j}\f(|y,s) \right]
\label{eq:FP_backward_pde}
\end{equation} 
This is the \emph{Backward Kolmogorov} equation for $\f$. Note the minus sign
in front $\di_s \f$. A mnemonic for the signs of the Backward vs. Forward
equation is that as $t$ increases $\f$ diffuses and so $\di_t$ and $\di^2_{x}$
have the same sign, but as $s$ increases, that is as $s$ approaches $t$, $\f$
anti-diffuses and they have opposite signs. 
  
The differential operator on the right-hand side of
\cref{eq:FP_backward_pde} occurs often in the study of SDEs and has its own
name.
\begin{defn}[Generator of an SDE] the Generator $A$ of $X$ is defined by:
$$
A[\psi(x)] = \lim_{t \searrow 0^+} \frac{\Exp[\psi(X_t)]  - \psi(x)}{t} ;  \quad X_0 =
x \in \R^n
$$
\end{defn} 
\begin{lemma} For an It\^o SDE as in \cref{eq:generic_Ito_SDE_Rn} 
$$
A[\psi(x)] = \sum_i F_i(x,t) \cdot \di_{x_i} \psi + \sum_{i,j} D_{ij}(x,t)
\di^2_{x_i x_j}\psi $$
\end{lemma}


\subsection{Stopping Times}
\begin{defn}
Let $\Fil(t)$ be some filtration, a random variable $\t$ is called a
\emph{stopping time} if 
$$
\{\o : \t(\o) \leq t\} \in \Fil(t) \, \forall t  
$$
\end{defn}
The colloquial way of describing stopping times is at any time we know
whether $\t$ has occurred or not. For a counterexample, the time that a Wiener
Process achieves its maximum over some interval is not a stopping time, since at any
given time, we do not know if the maximum has occurred or not. The most common
example of a stopping time is the first hitting-time, which is the first time
$X_t$ leaves or enters some set. 
\begin{thm}[First-Hitting time] Let $E \subset \R^n, E \neq
\phi$ be open or closed
 
then $$\t := \inf\{ t \geq 0 | X_t \in E\}$$ is a stopping time.
\end{thm}
 
The reason stopping times are very useful is that all the facts so far quoted
for It\^o calculus using integrals $\int_0^T \intd{W}$ remain true if $T$ is
replaced by a stopping time $\t$.
 
Also it allows us to link SDE's and PDEs using the generator, $A$, of the
diffusion: 
\begin{thm}[Dynkin's formula] Given $\t$ a stopping time, $\Exp[\t] < \infty$.
Then:
$$
\Exp[u(X_\t, \t)] =
u(x, 0) + \Exp\left[\int_0^\t \di_t u + A[u] \intd{s}
\right]
 $$
\end{thm}
This provides a link between PDEs and stochastic processes and allows us to go
back and forth in that we can find probabilistic results by
solving a PDE or we can approximate a PDE by simulating a stochastic process
and averaging.

\begin{ex}[\cite{Evansb} pg 99 - Expected hitting time to a boundary]
\label{ex:mean_hitting_time}
 Let $\O
\subset \R^n$ be a bounded open set with smooth boundary $\di \O$ then it is a
basic fact from PDEs theory that
\begin{equation}
\begin{cases}
-\frac{1}{2} \grad^2 u = 1  & \text{over } \O
\\
u =  0 &\text{on } \di \O
\end{cases}
\end{equation}
has a unique $C^\infty(\O)$ solution.

Let $X = W_t + x$ for any $x \in \O$ and define
 $$\t_x := \text{first time } X \text{ hits } \di \O$$
then the generator of $X$ is $A[\psi] = -\grad^2(\psi)/2$ and we will
have:

\begin{align*}
\Exp[u(X_\t)] - \Exp[ u(x_0)] =&
 \Exp \left[ -\frac{1}{2}\int_0^\t \grad_x^2 u(X_t) \intd{t} \right]
\\
=& \Exp \left[ - \int_0^\t 1  \intd{t} 
\right]
\\
=-&\Exp [\t]
\end{align*}
Finally, invoke $u$'s BCs, $u|_{\di E} = 0$, and $X$'s ICs, $X_0=0 = x$ to
conclude that $$ u(x) = \Exp [\t]$$
The solution to the PDE evaluated at $x$ is the expected exit time from
$E$ for an $X_t$  starting at $x$.
\end{ex}


\subsection{Numerical Simulation of SDEs}
In general SDEs, such as \cref{eq:generic_Ito_SDE_Rn}, cannot be solved
analytically and if one wants to obtain approximate paths from their solution,
one needs numerical methods.  

The theory of discretization for SDEs offers many numerical methods for
approximating \cref{eq:generic_Ito_SDE_Rn}, see \cite{Higham2001} for a popular
introduction, but here we will only describe the most basic method, the Euler-Maruyama scheme, which will suffice for our
purposes. In the Euler-Maruyama scheme, the approximate solution to
\cref{eq:generic_Ito_SDE_Rn} is computed at predetermined time-nodes,
$\{t_k\}_{k=0}^N$ with time intervals $\Delta t_k = t_{k+1} - t_{k }$, which
are often constant, $\Delta t_k = \Delta t\, \forall k$. Then given an initial
condition $X_0 = X(t_0)$, the approximate path, $\{X_k\}_{k=0}^N =
\{X(t_k)\}_{k=0}^N$ is obtained recursively via:

\begin{equation}
X_{k+1} = F(X_k,t_k)  \Delta t_k   + G(X_k,t_k) \xi_k \sqrt{\Delta t_k }
\label{eq:euler_maruyama_discretization_generic_Ito_SDE}
\end{equation}
where $\xi_k$ is an independent draw from the standard normal. 


\section{Parameter Estimation for SDEs}
\label{sec:estimation}
Parameter estimation for SDEs has a rich body of theory, (TODO: authoritative
citation - Susanne?)  and there are many mathematical techniques
available. Let us rewrite the SDE in \cref{eq:generic_Ito_SDE_Rn} in order to explicitly that the 
functions $F, G$ are parametrized by some parameter set, $\th$.
\begin{equation}
dX_t = F(X,t;\th) dt + G(X,t;\th) dW,
\label{eq:generic_Ito_SDE_Rn_parameterized}
\end{equation}
For example, in the case of the OU process, \cref{eq:OU_equation_generic},
$F(X,t;\th) = (\m - {X_t})/{\tc}$ and $G(X,t;\th) = \b$, the
parameter set is $\th = \{\m, \tc, \b\}$.

In the standard problem formulation of SDE parameter formulation, one has exact
observations, $\{x_n\}_{n=0}^N$ at times $\{t_n\}_{n=0}^N$ from a process $X_t$
satisfying \cref{eq:generic_Ito_SDE_Rn_parameterized} and one seeks to find the
values of the parameter set $\th$.

\subsection{Maximum Likelihood Estimation}
A fundamental method, both
practically and theoretically, for estimating parameters in an SDE is the {\sl Maximum
Likelihoood} (ML) method, which proceeds by seeking those parameters which
maximize the likelihood of the observed data, $\{x_n\}_{n=0}^N$. In particular
let $$L(\{x_n\}; \th) = \Prob[ X_0 = x_0\ldots X_n = x_n\ldots X_N = x_N |
\th]$$ be the joint probability of observing the data $\{x_n\}$ given the
parameter set $\th$, also known as the likelihood. Then the ML method seeks to
maximize $L$.

In the case of independent observations, the
likelihood is just the product of the individual probabilities of each observation. In the case of SDEs is only
slightly more complicated, due to the Markov nature of the stochastic process. In particular,
the likelihood becomes the product of the transition probabilities. Recall that
earlier we defined the transition probability, $\f(x,t| y,s)$ in
\cref{eq:transition_prob_defn}. We shall also write this as $\f_\th(x,t|
y,s)$ is we need to explicitly be reminded of $\f$'s dependence on the
parameter set, $\th$. With that we the likelihood of the observed $x_n$
becomes
\begin{equation}
L(\{x_n\}; \th) = \prod_{n=1}^{N} f_\th(x_n, t_n| x_{n-1}, t_{n-1})
\label{eq:SDE_discrete_likelihood}
\end{equation}
Here, we assume that $x_0$ is fixed, otherwise we would have to add a term
specifying the probability distribution of $X_0$.

In general the transition density for a generic SDE is impossible to find
analytically. There are several ways to approximate it numerically. The most
generic relies on the numeric solution of the Fokker-Planck PDE in
\cref{eq:generic_FokkerPlanck}, but that is quite expensive and suffers from the
'curse-of-dimensionality' for SDEs of higher dimension. 

In some simple cases, the transition density {\sl can} be calculated. The OU
example is computed above is one such a case, where the transition density is
\begin{align*}
f(x_n, t_n| x_{n-1} t_{n-1}) &=
 f(x_n, \Delta| x_{n-1} 0)\\& =
 \frac{1}{ \b \sqrt{\tc 2\pi(1 -  e^{-2 \Delta/\tc}})}
 	\cdot \exp\left(\frac{\left( x - \mu)  - (x_{0} - \mu) \cdot
 	 e^{-\Delta/\tc} \right)^2  } {\t \s^2  (1-e^{-2 \Delta/\tc})}
 	\right) 
\end{align*}
assuming that $\Delta_n = t_n-t_{n-1} = \Delta$ is constant for all $n$.
With this one can form the likelihood and solve analytically for the
maximizers, $\{\hat\m , \hat\tc, \hat\b \}$.
\begin{eqnarray} 
\hat{ \mu} &=& 
\frac{  \sum_{n=1}^{N } 
     \left( X_n - e^{-\frac{\Delta} {\hat \tc}} X_{n-1} \right)} 
	 { N( 1-e^{-\frac {\Delta} {\hat \tc}}) }
\\
e^{-\frac {\Delta}{\hat{\tc}} } &=& 
\frac { \sum_{n=1}^{N} 
			( X_n -  \hat \mu)(X_{n-1} -  \hat \mu) }
    {   \sum_{n=1}^{N } \left( X_{n-1} - \hat \mu
    \right)^2 }
\\
\hat\beta^2 &=&  
\frac{ 2  \sum_{n=1}^{N_k}  \left( X_n - \hat \mu - (X_{n-1} -
\hat \mu) e^{-\frac {\Delta} {\hat \tc}} \right)^2 } 
	  { N (1-e^{-2\frac {\Delta} {\hat \tc}}) \hat \tc}
\end{eqnarray}
The solution for the ML estimates is almost explicit. It requires one numerical
single-dimensional root-finding, which is an easy numerical task.

\subsection{Fortet Equation}
TODO

\subsection{Particle Filtering}
In {\sl Bayesian} approaches to parameter estimation, the unknown parameter,
$\th$ is also treated as a Random Variable, $\Th$, with some belief
distribution: $$
\rho(\th) = \Prob(\Theta = \th)
$$
then through applications of the Bayes formula, observations from the system are
used to update the belief distribution.


TODO: 

\subsection{Optimal Design}
\label{sec:optimal_design}
There are estimation problems, where the experimenter has some control over some
of the parameters in the model and may choose to set them as to facilitate the
estimation task. {\sl Optimal design} is the approach to design of statistical
experiments that is guided by optimizing some formal measure of the parameter estimates of the experiments, \cite{Pukelsheim2006}. For example, perhaps one would like to
perform linear regression with a polynomial model and one has some latitude over
at which points to evaluate the polynomial. Common criteria when selecting the
design, e.g the polynomial points, are minimizing the determinant or the trace
of the covariance matrix of the parameter estimators. The covariance matrix,
also known as the Fisher Information, however often depends on the very
parameters that one seeks to estimate.

An alternative to the Fisher Information as an objective for the formal design
of experiments is to use of concepts from Information Theory, \cite{MacKay2003}.
A related topic in the Machine Learning literature is called 'Active Learning',
e.g. see \cite{Cohn1996,Settles2010,Seeger2008}. In the third part of the
thesis, \cref{ch:optimal_design}, we will use the {\sl Mutual Information}
criteria as guideline in choosing stimulation to facilitate parameter
estimation. Thus we now define and discuss the Mutual Information between two
random variables $X, \Th$. For reference, we follow \cite{MacKay2003}.

\begin{defn}Mutual Information:
Given two random variables $X,\Th$ with joint probability density
$p(x,\th)$ and marginal densities $p(x), p(\th)$, the Mutual Information between
$X$ and $\Th$ is given by
\begin{equation}
I(X,\Theta) = \int_\Theta \int_X p(x,\th) \cdot \log \left(
\frac{p(x,\th)}{p(x)p(\th)}\right) \intd{x} \intd{\th}
\label{eq:mutual_info_defn}
\end{equation}
\end{defn}

It is obvious that if $X,\Th$ are independent than $I(X,\Theta) = 0$ and it can
be verified that $I\geq0$ and that it is maximized if $\Theta$ is a function of
$X$; that is, if the entropy of $\Th$ conditional on $X$ is zero. 
The mutual information, $I(X,\Th)$ represents the 'average
reduction in uncertainty about $\Th$ that results from learning the value of
$X$' \cite{MacKay2003}. This statement is formally correct if one takes
 'uncertainty' to mean the entropy of a random
 variable.
 
In order to make use of the Mutual Information in a parameter estimation
context, we need to specify a prior belief distribution over the  parameters.  
$$
\rho(\th) = \Prob(\Theta = \th)
$$
A prior distribution is standard and central concept in Bayesian statistics.
 
With that we are treating the unknown parameter, $\th$ like a random variable,
which we write as $\Th$. It is then natural to seek an experiment which
maximizes the Mutual Information between, $\Th$ and the observations $X$

To obtain the joint and margincal distributions of $\Th$ and $X$, we
need to recall Bayes' formula which relates the posterior of the parameters,
$p(\th|x)$ to the observations and the parameters' prior. 

\begin{equation}
p(\th|x) = \frac{L(x|\th)\rho(\th)}{\int_\Th L(x|\th)\rho(\th)}
\label{eq:bayes_formula}
\end{equation}
where $L$ is the likelihood of the observations.

Moreover, the marginal distribution of the parameters, is just its prior,
$p(\th) = \rho(\th)$, while the joint distribution of the parameters and the
observations is $$p(x,y) = L(x|\th)\rho(\th)$$.

while the $x$ marginal is $$p(x) = \int_\Theta L(x|\th)\rho(\th) \intd{\th}$$
Plugging the three expressions for $p(x,\th), p(x)$ and $p(\th)$ in the
definition, \cref{eq:mutual_info_defn}, gives:
\begin{equation}
I(X,\Th) = \int_\Theta \int_X L(x|\th)\rho(\th) \cdot 
\log \left( \frac{L(x|\th) }
				{\int_\Theta L(x|\th)\rho(\th) \intd{\th}  } \right)
\intd{x}\intd{\th}.
\label{eq:mutual_info_posterior_vs_observations} 
\end{equation} 
\Cref{eq:mutual_info_posterior_vs_observations}  is used in
\cref{ch:optimal_design}.


\section{Stochastic Optimal Control}
Optimal Control Theory has three main components - a state, $x$, a control,
$\a$, and an objective $J$ which is a functional of $\{x,\a\}$ and which we try
to either minimize or maximize. In {\sl Stochastic} Optimal Control, $x$ follows a
stochastic process, thus the objective, $J$ is most often expressed in terms of
some average or expectation over the random realizations of $x$. The general
theory of Optimal Control, as well as the subset dealing explicitly with random
systems, has had two main analytical techniques - {\sl the Maximum Principle}
and {\sl Dynamic Programing}, (a classic reference is \cite{Fleming1975}). The
Maximum Principle uses a a variational approach to characterize the optimal pair, $x_{opt}, u_{opt}$
optimizing $J$, while Dynamic Programing recursively builds up the optimal
solution with a backwards induction from the terminal conditions. Both
techniques have their advantages and disadvantages and we use both in the thesis. It turns out
that there is a close relation between the two approaches, which is well known
in the deterministic finite dimensional case, (\cite{Fleming1975,Evansb}. 

To set notation right, we consider the following functional
\begin{equation}
J[u] = \Exp_X \left[ \int_0^\tf L(X_t, \a_t) \intd{t} + M(X_\tf) \right]
\label{eq:generic_objective_functional} 
\end{equation}
over the realizations of $X_t$ governed by an It\^o SDE as in
\cref{eq:generic_Ito_SDE_Rn}, such that the drift $F$ and/or the diffusion
coefficient, $G$ are parametrized by the control $\a(t)$. Here $L$ is the
running cost function, which depends on the trajectory and the applied control,
while $M$ is the terminal cost function, which just depends on the value of the
trajectory at the terminal time, $t_f$. Here we assume that the terminal time is
a priori known, although this is not necessary in general. 

Given $J$ in \cref{eq:generic_objective_functional}, we then seek $\a$, which
maximizes it (for example).

\begin{equation}
\a^* = \argmax_{\a \in \Udomain} [ J[\a] ]
\label{eq:generic_optimization_statement}
\end{equation} 

To be mathematically correct, we should specify that the optimization in
\cref{eq:generic_optimization_statement} is done over the space, $\Udomain$ of
stochastic processes that are measurable with respect to the filtration
generated by the underlying Wiener process, $W_t$. Further constraints on
$\Udomain$ may be imposed by a specific problem.

The simple-looking \cref{eq:generic_objective_functional} can give rise to
different variations. For one, the final-time, $\tf$ may be variable. Or we may
face path or terminal constraints for $X_t$. In the stochastic context such
constraints are only enforceable in a probabilistic sense and they can easily
make the problem much more difficult. However we will not deal with either of
these complications and in the sequel we will assume that $\tf$ is fixed and
that $X_t$ faces no constraints other than that it satisfies its SDE.

Given the objective and the optimization equations,
\cref{eq:generic_objective_functional,eq:generic_optimization_statement}, the
Maximum Principle relies on the forward Kolmogorov (Fokker-Planck)
equation-based definition of the SDE expectation, while Dynamic Programing
relies on the relation between an SDE expectation and the backward Kolmogorov
equation.


\subsection{The Maximum Principle for Stochastic Optimal Control} 
\label{sec:maximum_principle_4_stochastic_control}
WARNING (TODO): IN the literature on stochastic control, there is an alternative
notion of the maximum principle, which is purely in the SDEs form and leads to
Backward SDEs (BSDEs), but this is not what we are talking about.

\vskip 10pt
The Maximum Principle uses a variational principle to characterize the optimal
control, $\a$ and the optimal {\sl transition } density. Note that the
expectation in the objective, \cref{eq:generic_objective_functional} can be written in terms of the foward
probability density of the state $X_t$:
\begin{align}
J[\a] =&  \Exp_X \left[ \int_0^\tf L(X_s, \a_s) \intd{s} + M(X_\tf) \right]
\notag \\
&=  \int_0^\tf\int_X L(x, \a_s) \cdot f(x,s|x_0,0) \intd{s}\intd{x} 
+ \int_X  M(x)\cdot f(x,\tf|x_0,0)\intd{x}.
\label{eq:generic_objective_functional_in_terms_of_forward_density}
\end{align}
As such the stochastic problem is reduced to a deterministic optimization
problem but for PDEs, given that $f$ follows the forward PDE in
\cref{eq:generic_FokkerPlanck}. 

Thus, for this thesis purposes, the Maximum Principle for Stochastic Optimal
Control is really a Maximum Principle for PDEs. Its variational argument is
similar in spirit to the Euler-Lagrange equations from the Calculus of
Variations; one can think of it as a generalization of the zero-tangent
rule (Fermat's Rule) for finding optima in single-variable calculus. In fact,
like both the Euler-Lagrange equations and Fermat's Rule, the Maximum Principle
provides necessary, but not sufficient conditions for an optimum.

Originally, the Maximum Principle was developed for finite-dimensional
deterministic systems, i.e.\ systems described by ODEs. In that context it is
known as the {\sl Pontryagin} Maximum Principle and for ODEs, the theoretical
results of existence and uniqueness of optimal controls are strongest. In the
infinite dimensional case, the theory is not as unified. TODO refer
more to Lenhart's chapter \cite{Lenhart2007}. Very recently, there has been a
series of publication on PDE control of the Fokker-Planck equation, see
\cite{Annunziato2010,Annunziato2013,Annunziato2014}. In particular,
\cite{Annunziato2014}, discusses the relation between the Dynamic Programing
approach to Stochastic Control and the approach based on PDE optimizaton of the 
Fokker-Planck equation.

The basic idea of the Maximum Principle is that one 'adjoins' the dynamics PDE
to the objective and introduces a Lagrange multiplier, which in this case is
called {\sl the adjoint state}.

Let us rewrite the governing SDE, \cref{eq:generic_Ito_SDE_Rn}, to explicitly
take the control  account for the control variable, $\a(t)$:
\begin{equation}
dX_t = F(X,t; \a) dt + G(X,t) dW,
\label{eq:generic_Ito_SDE_Rn_controlled}
\end{equation} 
Given the problems considered in this thesis, we only
illustrate  a one-dimensional SDE. Its corresponding forward density is
governed by the following Fokker-Planck equation: 
\begin{equation}
\di_t \f= -\di_{x } \left[ F (x,t;\a) \f(x,t) \right] +  
\di^2_{x} \left[ D(x,t) \f(x,t) \right]
\label{eq:generic_FokkerPlanck_controlled}.
\end{equation} 
where $D(x,t) = G^2(x,t)/2$. We assume that for all $\a(t)$, both the SDE and the
PDE have unique solutions. 

For notational convenience, we will write
\cref{eq:generic_FokkerPlanck_controlled} as
$$ \dot{\f} = \L_{\a} [\f] $$ where $\L_{\a}$ is the differential operator corresponding to the
right-hand side of the Fokker-Planck equation parametrized by the control $\a$.
For now we will assume that there are no BCs, and the domain of $X$ is all of $\R$.

As we already alluded, the key concept in Maximum Principle for PDEs is to
adjoin the dynamics, \cref{eq:generic_FokkerPlanck_controlled}, multiplied by
the adjoint state, $p$ to the objective,
\cref{eq:generic_objective_functional_in_terms_of_forward_density}
  
\begin{align*}
J[\a] =& \int_0^\tf\int_X L(x, \a_s) \cdot f(x,s) \intd{s}\intd{x} 
+ \int_X  M(x)\cdot f(x,\tf) \intd{x}
\\ &- \int_0^\tf\int_X p \cdot (\di_t f(x,s)  - \L_{\a} [f(x,s)] )
\intd{s}\intd{x}
\end{align*}
Since $f$ satisfies the PDE, we have added a term that equals zero. 
However, what this allows us to do is to 'transfer' the time and space
derivatives from $f$ to $p$. The reason why
that is productive is that we will then be able to form the 'variation'
of $J$ with respect to the control $\a$ and either set it to zero or use this as
a gradient. 

To illustrate, let us perform this 'transfer' explicitly - it is basically an
application of integration-by-parts; in larger dimension this is also commonly
called {\sl Green's identities}.  We have that $$
 \int_0^\tf   p \cdot \di_t f   \intd{t} =
  p\cdot f|_0^\tf - \int_0^\tf   \di_t p \cdot   f  \intd{t}
$$ which 'transfers' the time-derivative to $p$ and that $$ \int_X  p \cdot 
\L_{\a} [f ]  \intd{x} = \int_X  ( F  \di_x p + D \di_x^2 p) \cdot  f  \intd{x} =
\int_X   \Lstar_{\a} [p] \cdot f  \intd{x} $$ which transfers' the
space-derivative to $p$. In doing so, we have  naturally introduced 
 the differential operator $\Lstar$, which is the adjoint, in a Banach-space
 sense, to $\L$. Actually, we have already met $\Lstar$ before - it is the
 generator of the SDE in \cref{eq:generic_Ito_SDE_Rn_controlled}.

In the above manipulations, we have assumed that the double integrals have the
same value independent of the order of integration of the space and time
variables  and that $f$ and all its partials go to zero uniformly for $|x|$
large enough. If we had boundary conditions on $f$, those will come up in the
spatial terms.

With the above integration-by-parts done, we can write the objective as
\begin{align}
J[\a] =& \int_0^\tf\int_X L(x_s, \a_s) \cdot f(x,s) \intd{s}\intd{x} 
+ \int_X  M(x)\cdot f(x,\tf) \intd{x} \notag
\\ &- 
\int_X  \left[ p\cdot f|_0^\tf  +
    \int_0^\tf  (\di_t p  + \Lstar_{\a} [p]) \cdot f  \intd{s} \right] \intd{x} 
\label{eq:generic_objective_functional_in_terms_of_forward_density_adjointed}
\end{align}

Now we assume that we apply a small variation around a given control $\a$
\begin{align*}
\a_\e = \a + \e \da
\\
f_\e = f + \e \df
\end{align*}

With that the variation of the objective, $J$, with respect to the control
variation at the current $\a$ can be calculated as:
\begin{align}
\frac {dJ}{d\e} \Big|_{\e = 0} &= 
\int_0^\tf\int_X \grad_\a L(x_s, \a_s) \cdot \delta \a \cdot f(x,s) +
L(x_s, \a_s) \cdot \delta f(x,s)
\intd{s}\intd{x} 
\notag \\
&+ 
\int_X  M(x)\cdot \delta f(x,\tf) \intd{x} \notag
\\ &- 
\int_X    p\cdot \delta f(x, \tf)     \intd{x}  \notag \\+& 
    \int_X \int_0^\tf  (\di_t p  + \Lstar_{\a} [p]) \cdot \delta f 
    + \grad_\a F(x,t;\a) \cdot \delta \a \cdot  \di_xp \cdot f 
     \intd{s}
    \intd{x}
\label{eq:J_variation_PDE}
\end{align}
A few notes are required in order to better explain \cref{eq:J_variation_PDE}.
The initial conditions are considered fixed and as such $\delta f |_{t=0} \equiv
0 $, that is the variation in the control does not change $f(x, 0)$. This is
why only the term $p\cdot f|_{t=\tf}$ is retained from \cref{eq:generic_objective_functional_in_terms_of_forward_density_adjointed}.

Heuristically, we would like to infer from \cref{eq:J_variation_PDE} a gradient
with respect to the control, however we note that there are also variations with
respect to $f$ that make it impossible to do so. However, recall that $p$ is
our free variable - the Lagrange multiplier. Thus if we choose $p$
appropriately, we can eliminate $\delta f$ from the expression. Since we
have set up the problem with this in mind, this is now straight forward
to do - we let $p$ evolve (backwards) according to:
\begin{equation}
\begin{cases}
-\di_t p &= \Lstar[p] + L
\\
p(x, \tf) &=  M(x)  
\end{cases}
\label{eq:generic_adjoint_PDE}
\end{equation}

Thus, \cref{eq:J_variation_PDE} simplifies to
\begin{equation}
\frac {dJ}{d\e} \Big|_{\e = 0} =
\int_0^\tf\int_X \left[ \grad_\a L(x_s, \a_s) \cdot f(x,s) +
 \grad_\a F(x,t;\a) \cdot \di_xp \cdot f \right] \cdot \delta \a
     \intd{s}    \intd{x}
\label{eq:J_variation_PDE_simplified}
\end{equation}

From \cref{eq:J_variation_PDE_simplified}, we can infer that 

\begin{equation}
\grad_a J(x,t) = \grad_\a L(x, \a_t) \cdot f(x,t) +
 \grad_\a F(x,t;\a_t) \cdot \di_xp(x,t) \cdot f(x,t)  
\label{eq:J_gradient_wrt_control}
\end{equation}
can be considered as a pointwise gradient of the objective with respect to
changes in the control given the current control. It is then natural to claim
that setting that equal to zero will give us a necessary condition for an optimal
$\a$.  If setting \cref{eq:J_gradient_wrt_control} to 0 and solving for $\a$
is possible,  we would then  have an
expression of $\a$ in terms of the state, $f$ and the adjoint $p$, which we
could then input in their respective equations. Of course, this explicit
representation of $\a$ would not always be possible for incremental cost/rewards, $L$ or drift fields, $F$. Even if it were possible, we
would still have to solve the pair of now-nonlinear forward/backward equations
for $f,p$. It should be clear by now, why the Maximum Principle theory for PDEs
faces many practical challenges. 

% TODO: gradient descent vs fixed point iteration
There are two practical approaches to obtain actual numerical results given,
\cref{eq:J_gradient_wrt_control}, both of them iterative. 


{\sl If} one can
express $u$ in terms of $f,p$ and thus create a non-linear forward-backward system, by replacing $u$ in
the respectively forward and backward equations for respectively, $f$ and $p$,
but due to the coupled nature, this is usually unsolvable all at once. Thus
iterative methods are required. These come in two main flavours - 1) gradient
descent and 2) fixed point iteration. 


The fixed point approach is advocated in \cite{Lenhart2007}, for example, but only for simple pedagogic examples. 

Alternatively, we can apply a gradient-based optimization method. Since
\cref{eq:J_gradient_wrt_control} gives us a gradient, we can take the current
control $\a$ and increment it in the direction of $\grad_a J$ (if we
are maximizing)

As with all gradient-based optimization, the standard disclaimer about local
minima and optimization initialization applies, since there is no guarantee and
is usually not the case that the objective is convex with respect to the
control. 

Our derivations have not been very rigorous. More rigorous arguments can be
found in, e.g. (TODO) \cite{Fattorini1999,Borzi2012,Ahmed1981}.
  
 
\subsection{Dynamic Programing for Stochastic Optimal Control}
\label{sec:dynamic_programing}
Dynamic Programing uses backwards recursion to tabulate the optimal control
starting from the terminal time. The basic object in dynamic programing is the
value function, $v$. In order to introduce it, we first extend our definition
for the objective, $J$, to consider starting the state, $X_t$ at later
times, with different initial conditions.

The running cost-to-go corresponding to
\cref{eq:generic_objective_functional} is:
\begin{equation}
J[\a; x, t] = \Exp \left[ \int_t^\tf L(x_s, \a_s) \intd{s} + M(x_\tf) \right] ,
\quad X_t = x
\label{eq:generic_cost_to_go} 
\end{equation}
So that $J[\a; x_0, 0] = J[\a]$ is our original objective from
\cref{eq:generic_objective_functional}.
 
%TODO: change u-> \a for the control notation
We now introduce the {\sl  value function} , $v$,
defined by: 
$$
v(x,t) = \inf_{\a \in \Udomain} J[\a; x,t] 
$$
it is also called the optimal cost-to-go. 

We immediately note the terminal conditions on $v$:
\begin{equation}
v(x,\tf ) = M(x)
\end{equation}

That is if $X_\tf = x$, there is no more time for a control to be applied and no
more running cost to be incurred and we just incur the terminal cost
corresponding to wherever $X$ is now, i.e. $M(x)$.
  
It turns out that the value function, $v$, can be characterized as the solution
to the following non-linear PDE:
\begin{thm}[Stochastic Hamilton-Jacobi-Bellman (HJB)] 
\label{thm:stochastic_hjb}
\begin{equation}
\begin{cases}
-\di_t v(x,t) &=  \max_{\a(t) \in \Udomain(t)} \big\{ L(x,\a)  +
F(x,t,\a) \cdot \di_x v
\big\} + \frac{G^2(x,t)}{2} \di_x^2v
\\
v(x,\tf) &= M(x)
\end{cases} 
\label{eq:generic_HJB}
\end{equation}
\begin{proof}[Heuristic Derivation adapted from \cite{Evansb}] Suppose we are
at time $t$ and $X_t = x$.

Take a time increment $[t, t+h]$ and assume that during that time we apply a
constant control $\a$ and subsequently we apply the the optimal control $\a^*$.
The running-cost will then break down as: 
$$
J[u; x,t] = \Exp \left[ \int_t^{t+h} L(X_s, \a_s) \intd{s}  + v(X_{t+h},
t+h) \right] $$

Since $v(x,t) = \inf J(u; x,t)$, we must have that:
$$
v(x,t) \leq  \Exp \left[ \int_t^{t+h} L(X_s, \a_s) \intd{s}  + v(X_{t+h},
t+h) \right] $$
or, rearranging, that
\begin{align*}
0 \leq^&  \Exp \left[ \int_t^{t+h} L(X_s, \a_s) \intd{s} \right]  
+ \underbrace{\Exp \left[ v(X_{t+h}, t+h) - v(x,t) \right]}_{\Exp\left[ dv
\right]}
\end{align*}
Now, $\Exp\left[ dv \right]$ can be expressed using Dynkin's Formula, as:
$$
\Exp[dv] = \int_t^{t+h} \di_t v +  F\cdot \di_x v + \frac{G^2 }{2}\cdot \di_x^2
v \intd{s} $$
Plugging that back, we get
$$
0 \leq \int_t^{t+h} L(X,u) +  \di_tv +  F\cdot \di_x v + \frac{G^2 }{2}\cdot
\di_x^2 v \intd{s} $$
Taking $h \ra 0$ we get 
$$
0 \leq  L(x,u) +  \di_tv(x,t) +  F\cdot \di_x v(x,t) + \frac{G^2 }{2}\cdot
\di_x^2 v(x,t) $$
and we conjecture that for the actual optimal control, the inequality becomes an
equality:
\begin{equation}
\begin{cases}
0 &=   L(x,u^*)+ \di_t v(x,t) +
 F(x,t,\a^*)\cdot \di_x v(x,t) + \frac{G^2(x,t)}{2}\cdot \di_x^2 v(x,t)
\\
\a^*  &= \argmax_{\a \in \Udomain(t)}  
\big\{L(x,\a) + F(x,t,\a)\cdot \di_x v \big\}
\end{cases}
\end{equation}
\end{proof}
\end{thm}

It turns out that rigorous proofs of Theorem
\ref{thm:stochastic_hjb} , in particular specifying the exact
mathematical meaning for a solution, $v$, to HJB PDE are quite complex and beyond the scope
of this text. Some references include \cite{Krylov2008,Fleming2006}. We will
assume that solving numerically solving \cref{eq:generic_HJB} is sufficient. 

\subsection{Numerical Solutions to PDEs - Finite Difference Methods}
Both the variational approach of the Maximum Principle and the backwards
induction of Dynamic Programing result in having to solve PDEs in order to
obtain the optimal control. For practical purposes, solving these PDEs requires
numerical discretization. In all cases the PDEs are {\sl
parabolic} (see \cite{Press1992}), which in general can be written as 
\begin{equation}
\di_t f = \L[f(x,t)]
\label{eq:generic_parabolic_PDE}
\end{equation}
for some differential operator, $\L$. 
In one spatial dimension, a finite difference discretization of
\cref{eq:generic_parabolic_PDE} is to select a set of space- and time- nodes
$\{x_i\}, \{t_k\}$ and approximate $f$ by solving for $f(x_i, t_k)$, given the
initial conditions $f(x_i, t_0)$ and possibly boundary conditions.

A classic technique for parabolic PDEs is the
{\sl Crank-Nicholson} scheme, which time discretizes
\cref{eq:generic_parabolic_PDE} as
$$
\frac{f(x_i, t_{k+1})- f(x_i, t_k)}{ t_{k+1}-t_k}
=
\frac 12 \left(\L[f(x_i, t_{k+1}  )] + \L[f(x_i, t_k)]
\right)
$$
and then solves for the resulting linear system before stepping
iteratively forward in time. See Ch 19 in \cite{Press1992} for a brief
discussion the theoretical properties of the Crank-Nicholson method.   

This solution approach requires a finite spatial domain. If the spatial domain
is theoretically infinite as it may be in some cases, we need to truncate it and
apply some reasonable boundary conditions at the artificial boundary which
approximate the solution of the theoretically infinite space. We will show
details of how this is done in the context of each specific problem
discussed in the thesis.  
 
\section{Mathematical Models in Neuroscience}
\label{sec:math_models_in_neuroscience}
We now describe in detail the basic mathematical model of a neuron that will be
used in the sequel. 

An introduction to mathematical models in neuroscience is given in Gerstner et
al., \cite{Gerstner2014}, also available online. The book Stochastic Methods in
Neuroscience, \cite{Laing2009} gives a nice overview of several current research
applications of stochastic techniques to neuroscience.

Neurons' most important property is their membrane potential - the electric
potential difference between the cell's interior and its surroundings. Neurons
relay information by means of voltage spikes - sudden sharp increases in their
membrane potential, which are then transmitted to other neurons that are
chemically or electrically connected to the spiking neuron. The information
content is thought to be contained in the timing of the spike, or equivalently
in the length of the time-interval between subsequent spikes. In the simplest
case, this can be thought of as a {\sl firing rate} - the average number of
spikes per time interval, but more complicated coding schemes are hypothesized
to exist. A spike can be triggered by spikes from impinging neurons or, on the
front line, for sensory neurons spikes are triggered by external stimuli, for
example light photons in the case of visual reception neurons. 

Given a small positive stimulus, the voltage of a neuron increases linearly, but
then relaxes back down to its equilibrium, pre-stimulus, level. However, if the
stimulus is sufficiently strong or if there are many small stimuli in a
sufficiently short interim, the voltage will then go through a large
 non-linear stereotypical excursion before resetting back down to its equilibrium
 - this is a spike. The physical underpinnings of this are thought to be ion
 channels with different time-scales which act excitatory on very short time
 scales and inhibitory on slightly larger ones (a spike unfolds on the order of
 1-2 milliseconds). The first successful mathematical model describing this
 phenomena is the famous Hodgkin-Huxley (HH) model, which forms a system of 4
 ODEs, one for the membrane voltage and three for the dynamics of the ion
 channels. From a practical point of view however, the HH model of
 spike-dynamics is quite complicated if one is only interested in spike timings,
 rather than detailed ion channel dynamics. Thus there have been several
 reductions in the model. The standard approach is to keep the voltage equation
 as well as one more equation for a 'recovery' variable which works on a slower
 scale than the voltage in order to produce the very sharp up-and-down voltage
 excursions. Two popular examples of such reductions are the Fitzhugh-Nagumo
 model and the Morris-Lecar model. The Morris-Lecar model
 is 2-dimension ODE for the variables $v(t), w(t)$, which reads:
\begin{equation}
\left\{
\begin{array}{ccl}
\dot{v}(t)  &=& \frac{1}{C}\Big(-g_{Ca}m_\infty(v) (v-V_{C_a}) -
g_K w (v-V_K) \\ && 
-g_L(v-V_L)+I(t)  \Big) \\
\dot{w}(t)&=&\left(\alpha(v)(1-w) - \beta(w)w\right)
\end{array}
\right.
\label{eq:ML_original_deterministic}
\end{equation}
where the auxilliary functions, $m_\infty, \alpha, \beta$ are given by:
\begin{eqnarray*}
m_\infty(v)&=&\frac{1}{2}\left(1+\tanh\left(\frac{v-V_1}{V_2}\right)\right),\\
\alpha(v) &=& \frac{1}{2}\phi \cosh\left(\frac{v-V_3}{2V_4}\right)\left(1+\tanh\left(\frac{v-V_3}{V_4}\right)\right),\\
\beta(v) &=& \frac{1}{2}\phi \cosh\left(\frac{v-V_3}{2V_4}\right)\left(1-\tanh\left(\frac{v-V_3}{V_4}\right)\right).
\end{eqnarray*}
TODO: Verify ML is right. 
The term, $I(t)$ represents current applied on the neuron, either from natural
external stimulation, an experimental control or impingent neurons. 

In \cref{sec:morris_lecar_control}, we describe how the Morris-Lecar
deterministic ODE of \cref{eq:ML_original_deterministic} is extended to an SDE.   
 
The 2-dimensional models like Morris-Lecar are much easier to work with
mathematically and experimentally, for parameter estimation, for example, but
they still expend a lot of effort in describing the details of a neural spike.
If one is only interested in the timing of the spike then yet another
simplification is to keep only the voltage dynamics and then arbitrarily declare
a spike has occurred whenever the voltage crosses some appropriate threshold,
$v_{thresh}$. For analytical purposes this turns out to be both convenient and
satisfactory. The basic integrate-and-fire model then is just
\begin{equation}
\begin{gathered}
\dot{v} = \left(I(t) - \frac{(v - \m)}{\tc} \right) \intd{t} 
\\
v(\ts) = v_{thresh} \implies  
\begin{cases}
v(\ts^+) = 0 &  
\end{cases}
\end{gathered}
\label{eq:deterministic_IF}
\end{equation}

In reality, the external stimulation to a neuron is highly erratic and
unpredictable. When the randomness is mostly in the input, $I$,
one can approximate this by adding to the input $I$ a white, Gaussian
noise, in the terminology of this thesis this is just a term proportional to
a Wiener process increment, $\beta dW_t$.

Thus the final model of the basic spike-generation mechanism that accounts for
its most important aspects yet retains analytical tractability is the noisy
leaky-integrate-and-fire model:
% TODO: tau_c -> tau
\begin{equation}
\begin{gathered}
dX_t = \left(\a(t) - \frac{(X_t - \m)}{\tc} \right) \intd{t} + \b \intd{W_t},
\\
X(0) = 0,
\\
X(\ts) = \xth \implies  
\begin{cases}
X(\ts^+) = 0 &  
\end{cases}
\end{gathered}
\label{eq:X_evolution_uo}
\end{equation}
That is $X_t$ follows an OU process, but upon reaching a pre-determined
threshold, $\xth$, a 'spike' is deemed to have occurred and the process is
'reset' to an initial value, here $0$.

Alternatives to the hard-threshold integrate-and-fire model are so-called
'soft-threshold' models which use a {\sl hazard} function which is akin to the
intensity of a Poisson Process to determine the spike time. The hazard function
is increasing in voltage, thus higher voltages imply higher likelihood of
spiking. The hard-threshold model can be seen as a special case of the
soft-threshold model, given the hazard function which is 0 below the threshold
and is infinite above the threshold. We will not address hazard-function based
spiking models in the rest of the thesis. 
