\chapter{Mathematical Background}
\label{ch:math_background}
\input{../OptEstimate/local_style.sty}
Here we collect a list of mathematical tools that are used in the thesis

We cite amongst others Oksendal for SDEs \cite{Oksendal2007} and Fleming and
Rishel for Optimal Control \cite{Fleming1975}.

A very readable introduction to the field of both SDEs and Optimal Control are
the online notes of Professor L. Evans \cite{Evansa,Evansb}. We have also used
Jacobs as our main tutorial on first-passage times for SDEs \cite{Jacobs}. 
 
\section{Stochastic Differential Equations}
\label{sec:SDEs}
In view of our ultimate goals, we will restrict ourselves to  stochastic
processes whose sample paths have continuous paths, i.e.\ to SDEs driven by
Brownian motion.  

Since the reader is more likely well familiar with the material in this section,
we will not provide proofs, but only state the results with a view towards
establishing the notation for the sequel.

We will assume that the reader is familiar with the following concepts:
\begin{itemize} 
  \item a probability space, $\{\O, \sAlg, P\}$ consisting of a
  probability space, a sigma-algebra and a probability measure
  \item a continuous-time stochastic process, $X_t$
  \item a filtration $\Fil(t)$ and in particular the filtration generated by a
  stochastic process.
\end{itemize}

\subsection{The Wiener Process and the Ito Integral}
The Wiener Process is the fundamental building block of the stochastic calculus,
it is often called Brownian Motion and we will denote it $\{W_t\}_{t\geq 0}$. It
satisfies:
\begin{defn}Wiener Process, $W_t$:
\begin{enumerate}
  \item $W_0 = 0$
  \item $W_t - W_s = N(0, |t-s|)$ , i.e. normally distributed increments with
  mean 0 and variance $|t-s|$)
  \item $\forall \{t_i\}_1^N, \quad \{W_{t_i} - W_{t_{i-1}} \}_2^N \sim$
  independent, i.e $W_t$ has independent increments
\end{enumerate}
\end{defn}
The fact that the finite incremental distributions suffice to specify a unique
continuous-time stochastic process is known as Kolmogorov's extension theorem.
Now, we collect a few more relevant properties of $W_t$:
\begin{enumerate}
  \item The sample paths $W_{[0,\infty)}(\o)$
are almost surely (a.s.) continuous and are in fact Holder continuous for any
exponent $\g < 1/2$
\item The sample paths $W_{[0,\infty)}(\o)$ are nowhere differentiable
\item $W_t$ is a Markov process: $\Prob[W_t \in B | \s(W_{s' \leq s })] =
\Prob[W_t \in B \,| \,W_s]$, for any Borel set, $B \subset \R$, where $\s(W_{s'
\leq s })$ is the filtration generated by the $W_t$
\end{enumerate}



We now turn to defining stochastic integrals based on the Wiener Process.
 
\begin{defn} Progressively Measurable Functions:

Let $\Fil_t$ be the filtration generated by the Wiener Process.

Let $X_t$ be a stochastic process which is $\Fil_t$-measurable $\forall t$ and
which is jointly measurable in $(t,\o)$. We call such an $X_t$
\emph{progressively measurable}
\end{defn}

\begin{defn} $\Ltwopm, \Lonepm$

We define $\Ltwopm[0,T]$ as the space of all progressively measurable
$X$ such that
\begin{equation*}
\Exp \left[ \int_{[0,T]} X_t^2 \intd{t} \right] < \infty
\end{equation*}

Similarly, we define $\Lonepm[0,T]$ as the space of all progressively measurable
$X$ such that
\begin{equation*}
\Exp \left[ \int_{[0,T]} X_t \intd{t} \right] < \infty
\end{equation*} 
\end{defn}

$\Ltwopm$ will be the class of functions for which the Ito integral is
well-defined. 
\begin{defn} Ito Integral:
\label{defn:ito_integral}

Let $P^n := {a = t^n_1 \ldots t^n_{m_n} = b}$ be a partition of the interval
$[a,b] \subset [0, \infty)$. Let $|P_n| = \sup_i|t_i - t_{i-1}|$. 
Take $|P_n| \rightarrow_n 0 $ and consider an $ X_t \in \Ltwopm[a,b]$,

then
\begin{equation}
\int_{[a,b]} X_t \intd{W} := \lim_{n \rightarrow \infty}  
\sum_{i=1}^{m_n} X_{t_i}\left( W(t_{i+1}) - W(t_{i})\right)
\end{equation}

\end{defn}

To be precise, our definition is actually a theorem, and the real
definition is one that uses step functions and passes to the limit.  Also we
will write the limits of integration $\int_0^T \cdot  \intd{W}$ or $\int_{[0,T]}
\cdot  \intd{W}$ interchangeably.
% To justify def'n \ref{defn:ito_integral}, we recall:
% \begin{lemma}[Quadratic Variation]
% \begin{equation}
% \lim_{n \rightarrow \infty} \sum_{i=1}^{m_n} \left( W(t_i) - W(t_{i-1})\right)^2
% = b-a \quad \text{in }  L^2(\O)
% \end{equation}
% \end{lemma}
% The above can be read, heuristically as $\lim \sum (\Delta W)^2 \rightarrow \sum
% \Delta t$, which is the meaning behind the colloquial $\intd{W} =
% \sqrt{dt}$.

Again, we state without proof a few interesting properties of $\int X \intd{W}$:
\begin{thm} Ito Integral Properties

\begin{enumerate}
  \item $\Exp[\int_0^T X \intd{W} ] = 0$ 
  \item $\Exp[\left(\int_0^T X \intd{W}\right)^2 ] = \Exp[\int_0^T X^2
  \intd{t}]$
  \item $I(t) = \int_0^t X \intd{W} $ is a martingale 
  \item $I(t) = \int_0^t X \intd{W} $ has continuous sample paths
\end{enumerate}
\end{thm}

\subsection{Ito SDEs and Ito's Lemma}
\begin{defn}[Ito SDE]

We write
\begin{equation}
dX_t = Fdt + G dW
\end{equation}
on $0 \leq t \leq T$, if $X$ is a real-valued stochastic process satisfying:
\begin{equation*}
X(r) = X(s) + \int_{[s,r]} F \intd{t} + \int_{[s,r]} G \intd{W}
\quad 0\leq s \leq r \leq T
\end{equation*}
for some $F \in \Lonepm[0,T]$ and $G \in \Ltwopm[0,T]$
\end{defn}

We are now ready to present the celebrated Ito Lemma which is the chain-rule of
stochastic calculus:

\begin{thm}[Ito Lemma]

Suppose $X$ satisfies the stochastic differential $dX_t =
Fdt + G dW$ as above and take $v(x,t) \in C^{2,1}[ \R \times [0,T]]$.

Set $Y(t) = v(X,t)$
\\
then
$$
dY =  \left( \di_t v + \di_x v \cdot F + \di^2_x v \cdot \frac{G^2}2 \right)
\intd{t} + \left(   \di_x v\cdot G  \right)\intd{W}
$$
\end{thm}

% We now present two examples, one fundamental and one more involved:
% \begin{ex}[pg 67 in \cite{Evansa}] The 'Stochastic' Exponential:
% 
% Let $Y = \exp(\l W - \l^2 t / 2)$. In the notation of the above Thm, we have:
% $dX = dW, F=0, G=1$ and $u(x,t) = \exp(\l x - \l^2 t / 2)$.
% 
% then
% \begin{align*}
% dY =& 
% \left( \di_t u + \di_x u \cdot F + \di^2_x u \cdot \frac{G^2}2 \right)\intd{t} +
% \left(   \di_x u \cdot G  \right)\intd{W}
% \\
% =&
% \left( -\frac{\l^2 t}{2}\exp(\l x - \l^2 t / 2) + \l^2\exp(\l x - \l^2 t / 2)  \right)\intd{t}
% \\ 
% &+ \left(   \l \exp(\l x - \l^2 t / 2) \right)\intd{W}  
% \\
% =&  \l \left( \exp(\l x - \l^2 t / 2) \right)\intd{W}  
% \\
% =& \l Y dW
% \end{align*}
% \end{ex}
% 
% \begin{ex}[pg. 67 in \cite{Evansa}] Hermite Polynomials:
% 
% Let $h_n(x,t) = \frac{(-t)^n}{n!} e^{x^2/2t} \di^n_x \left( e^{x^2/2t} 
% \right)$ be the $n$th Hermite polynomial
% \\
% then
% $$
% \int_0^t h_n(W, t) \intd{W} = h_{n+1}(W(t), t)) 
% $$
% i.e. in Ito calculus, the Hermite polynomials are like the power monomials
% $\frac{t^n}{n!}$ in the ordinary calculus.
% \begin{proof} Since
% $$
% d_\l^n [\exp(- \frac{(x-\l t)^2}{2t} ] |_{\l=0} =
%  (-t)^n d_x^n [ \exp(-\frac{x^2}{2t})] $$
% We will have:
% \begin{align*}
% d_\l^n [\exp(\l x - \l^2 t / 2) ] |_{\l=0} &= (-t)^n  e^{x^2/2t} d_x^n [
% \exp(-\frac{x^2}{2t})]
% \\
% &= n! h_n(x,t)
% \end{align*}
% Thus we can expand $\exp(\l x - \l^2 t / 2) $ in a Taylor series at $\l=0$ as:
% $$
% \exp(\l x - \l^2 t / 2) = \sum_n \l^n h_n(x,t)
% $$
% Now recall $Y$ from the previous example:
% \begin{equation*}
% Y(t) = \exp(\l W(t) - \frac{\l^2 t }2) = \sum_n \l^n h_n(W(t),t)
% \end{equation*}
% which satisfies:
% \begin{equation*}
% Y(t) = 1 + \l \int Y \intd{W}
% \end{equation*}
% Naturally we will plug the series expansion in the SDE to obtain:
% \begin{align*}
% \sum_{n=0}^{\infty} \l^n h_n(x,t) =&
% 1 + \l \int_0^t  \sum_{n=0}^{\infty} \l^n
% h_n(x,t)\intd{W}
% \\
% =& 1 + \sum_{n=1}^{\infty} \l^n\int_0^t 
% h_{n-1}(x,t)\intd{W} \quad {\textrm{bump $n$ up by one}}
% \end{align*}
% Equating the coefficients for $\l^n$ on both sides leads to the result
% \end{proof}
% \end{ex} 

Finally, we are in a position to define an Ito SDE:

\begin{defn}[Ito SDE] Let
$F(x,t):\R\times [0,T]\ra \R,  G(x,t):\R\times [0,T]\ra \R,$ be given functions.
We say that a stochastic process $X$ satisfies:
\begin{equation}
dX =F dt + G dW
\end{equation} 
over $[0,T]$ if
\begin{enumerate}
  \item $X$ is progressively measurable wrt. $\Fil_t$
  \item $F(X,t) \in \Lonepm$
  \item $G(X,t) \in \Ltwopm$ 
  \item $X_t = X_0 + \int_0^t F(X_s, s) \intd{s} + \int_0^t G(X_s, s) \intd{W_s}$
\end{enumerate}
\end{defn}

% \begin{ex} The solution to 
% $$
% \begin{cases}
% dX = F(t)X dt + G(t) X dW
% \\
% X_0 = 1
% \end{cases}
% $$
% is 
% $$
% X_t = \exp\left( \int_0^t F(s) - \frac{G^2(s)}{2} \intd{s} + \int_0^t G
% \intd{W} \right) $$
% \begin{proof}
% Let $Y = \int_0^t f(s) - \frac{g^2(s)}{2} \intd{s} + \int_0^t g
% \intd{W} $
% and guess that $X = e^Y$
% then, using Ito's lemma:
% \begin{align*}
% dX =& (\di_y [e^y] dY + \frac{1}{2}\di^2_y[e^y]  g^2 dt
% \\ =&
% (e^y  f dt - e^y g^2 / 2) dt + e^y g dW + \frac{1}{2} e^y \cdot  g^2 dt
% \\ =& e^y \left( f dt +g dW \right)
% \\ =& X \left( f dt +g dW \right)
% \end{align*}
% \end{proof}
% \end{ex}

% \begin{ex}[Brownian Bridge, pg 80 in \cite{Evansa}] The solution of the SDE
% $$
% \begin{cases}
% dB = -\frac{B}{1-t} dt + dW
% \\
% B_0= 0
% \end{cases}
% $$
% is
% $$
% B(t) = (1-t) \int_0^t \frac{1}{1-s} dW
% $$
% \begin{proof}
% It is not clear how to apply Ito's lemma to this problem, so instead we just
% directly calculate the differential:
% \begin{align*}
% B(t^+) - B(t) =& (1-t^+) \int_0^{t^+} \frac{1}{1-s} dW - (1-t) \int_0^t
% \frac{1}{1-s} dW
% \\ =&
% (1-t^+) \int_t^{t^+}\frac{1}{1-s} dW  - (t^+-t) \int_0^t
% \frac{1}{1-s} dW
% \\ \approx&
% \int_t^{t^+} dW - \Delta t \frac{B(t)}{1-t}  
% \end{align*}
% \end{proof}
% \end{ex}
% Actually we can say a bit more about the Brownian bridge, in particular we
% will calculate $\Exp[B^2]$
%  \begin{ex}[Evans .37]
% \begin{align*}
% \Exp[B^2] =& \Exp\left[\left( (1-t) \int_0^t \frac{1}{1-s} dW \right)^2 \right]
% \\
% =&(1-t)^2 \Exp\left[ \int_0^t\left(  \frac{1}{1-s}\right)^2 \intd{s} \right]
% \\
% =&(1-t)^2 [ \frac{1}{1-t} - 1 ]
% \\
% =&(1-t) - (1-t)^2
% \end{align*}
% 
% Further note that  $\lim_{t\ra 1^-}\Exp[B^2] = 0$ and that $B$ is a martingale
% and so its first moment is always equal to its initial value, $0$. So indeed the 2nd
% moment is the variance and with the variance converging to zero, we can use
% Chebyshev's inequality to conclude that $\lim_{t\ra 1^-}B(t) = 0 $(a.s). This
% justifies the name of the Brownian bridge - it is a normally distributed RV.
% which is clamped to equal 0 at both $t= \{0,1\}$.
% \end{ex}

As an example we will discuss the Ornstein-Uhlenbeck process, which is the
basis for the models we will face later on.
%TODO: Decide on whether it is (m-x)/t or m - x/t, once and for all!
\begin{ex}[O-U Process] Let $X$ follow:
\begin{equation}
dX = \left( \frac {\m -X_t}{\tc} \right) dt + \b dW
\label{eq:OU_equation_generic}
\end{equation}
with an initial condition, $X_0$, which may be an arbitrary distribution
independent of the Wiener Process, $W$.
We can solve this as follows:
%TODO: Solve OU with this parameter set:
\begin{align*}
dX =& \left( \frac {\m -X_t}{\tc} \right)  dt + \b dW
\\
dX + \frac {X_t}{\tc} dt=&  \frac\m\tc dt + \b dW
\\
e^{t/\tc} dX + e^{t/\tc}\frac {X_t}{\tc} dt
=& e^{t/\tc}\frac\m\tc dt + \b e^{t/\tc} dW
\\
Xe^{t/\tc} - X_0 
=& \int  e^{t/\tc}\frac\m\tc dt +  \int \b e^{t/\tc} dW
\\
X_t =& e^{-t/\tc} X_0 + \m(1-e^{-t/\tc}) +  \frac{\sqrt{\tc}\b
e^{-t/\tc}}{\sqrt{2}} W(e^{2t/\tc}-1)
\end{align*}
which means that $X$ forgets its initial conditions exponentially fast and
converges to a normal random variable with mean $\m\tc$ and variance
$\tfrac{\tc \b^2}{2}$
\end{ex}

At the end of this sub-section, we state Ito's Lemma with multiple Wiener
processes and then for a multidimensional state.

\begin{thm}[Ito Lemma for $dW \in  \R^{m}$] Suppose $X$ satisfies the stochastic
differential $dX_t = F dt + G dW$, where $ F \in \R, G \in \R^{1\times m}$ and
$dW = \left(dW^{(i)}\right) \in  \R^{m}$ is a vector of independent Wiener
Processes. Take $u(x,t) \in C^{2,1}[ \R \times [0,T]]$. Let $D :=\left[{\sum_k
G_{k} G_{k} } \right] \in \R$

Set $Y(t) = u(X,t)$
\\
then
$$
dY =  \left( \di_t u +  \di_{x} u \cdot F + 
  \di^2_{x} u \cdot D \right)
\intd{t} +
 \left(  \di_{x} u  \cdot  G \right) dW  
$$
\end{thm}

\begin{thm}[Ito Lemma for $X \in R^n$]
Suppose $X$ satisfies the stochastic differential $dX_t = F dt + G dW$, where $
F \in \R^n, G \in \R^{n\times m}$ and $dW = \left(dW^{(i)}\right) \in  \R^{ 
m}$ is a vector of independent Wiener Processes. Take $u(x,t) \in C^{2,1}[ \R^n
\times [0,T]]$. Let $D_{ij} := \tfrac{1}{2} \left[{\sum_k G_{ik} G_{jk} }
\right]_{ij} \in \R^{n\times n} $

Set $Y(t) = u(X,t)$
\\
then
$$
dY =  \left( \di_t u + \sum_i \di_{x_i} u \cdot f_i + 
\sum_{i,j} \di^2_{x_i x_j} u \cdot D_{ij} \right)
\intd{t} +
 \left(  \sum_{ij} \di_{x_i} u  \cdot G_{ij} dW^{(j)} 
\right)$$
\end{thm} 

\subsection{Fokker-Planck and Kolmogorov's Backward Equations}
The Fokker-Planck equation and Kolmogorov's Backward equation describe the
forward (resp. backward)  evolution of the probability density of $X_t$

For the rest of this section we will work in $ \R^n$, i.e. $X$ will be an $n-$
dimensional vector that satisfies the stochastic differential 
\begin{equation}
dX_t = F(X,t) dt + G(X,t) dW,
\label{eq:generic_Ito_SDE_Rn}
\end{equation}
$F \in \R^n, G \in \R^{n \times m}$,
$dW = (dW^{(i)}) \in  \R^{  m}$ is an $m$-dimensional Brownian motion and we
write $D_{ij} :=
\tfrac{1}{2} 
\left[{\sum_k G_{ik} G_{jk} } \right]_{ij} \in \R^{n\times n} $.

Let 
\begin{equation}
 \f(x,t| y,s) \intd{x} =  \Prob[X_t \in \intd{x} | X_s = y]
 \label{eq:transition_prob_defn} 
 \end{equation}
be the transition probability density. Then $\f$ satisfies:
\begin{equation}
\di_t \f= -\sum_i \di_{x_i} \left[ F_i(x,t) \f(x,t) \right] 
+ 
\sum_{i,j}  \di^2_{x_i x_j} \left[ D_{ij}(x,t) \f(x,t) \right]
\label{eq:FP_pde}
\end{equation}
This is called the \emph{Fokker-Planck} or \emph{Forward Kolmogorov} equation.
It can also be seen as a continuity euquation or a cosevation of probability
equation. To this end define the probability current, $\p \in \R^n$ as:
$$
\phi_i =   F_i(x,t) \f(x,t) 
+ 
\sum_{j}  \di_{x_j} \left[ D_{ij}(x,t) \f(x,t)\right]
$$
then the Fokker-Planck equation, \cref{eq:FP_pde}, can be written as:
$$
\di_t \f= -\grad \cdot \phi
$$
which just says that the change in probability is the
difference between the flow in and the flow out. 

Initial conditions for $\f$ are given by the distribution of $X_s$, if $s=0$
then this is the initial distribution of $X$. Boundary conditions (BCs) depend
on the domain of $X$ and what happens to $X$ once it hits its boundary. If the domain
is all of $\R^n$ then we only insist that $\lim_{|x| \ra \infty} \f = 0$. If the
domain has boundaries, then there are two common scenarios which we will also
encounter in the sequel:
\begin{enumerate}  
  \item absorbing BCs
  \item reflecting BCs
\end{enumerate}
At an absorbing boundary, the particle $X$ is removed and $\f=0$ there.
In this situation, we will not have conservation of probability and the integral
of $\f$ over $X$'s domain will monotonically decrease.

At a reflecting boundary, the particle $X$ bounces back into its domain and we
will have that the probability current $\p\cdot n = 0$ where $n$ is the outward
normal at the reflecting boundary.

We now consider $\f$ as functions of $y,s$, holding $x,t$ fixed, then
\begin{equation}
-\di_s \f= \sum_i  \left[F_i(y,s) \cdot \di_{y_i}\f(|y,s) \right] 
+ 
\sum_{i,j}   \left[ D_{ij}(x,t) \di^2_{y_i y_j}\f(|y,s) \right]
\label{eq:FP_backward_pde}
\end{equation} 
This is the \emph{Backward Kolmogorov} equation for $\f$. Note the minus sign
in front $\di_s \f$. A mnemonic for the signs of the Backward vs. Forward
equation is that as $t$ increases $\f$ diffuses and so $\di_t$ and $\di^2_{x}$
have the same sign, but as $s$ increases, that is as $s$ approaches $t$, $\f$
anti-diffuses and they have opposite signs. 
  
The differential operator on the right-hand side of
\cref{eq:FP_backward_pde} occurs often in the study of SDEs and has its own
name.
\begin{defn}[Generator of an SDE] the Generator $A$ of $X$ is defined by:
$$
A[\psi(x)] = \lim_{t \searrow 0^+} \frac{\Exp[\psi(X_t)]  - \psi(x)}{t} ;  \quad X_0 =
x \in \R^n
$$
\end{defn} 
\begin{lemma} For an Ito SDE as in \cref{eq:generic_Ito_SDE_Rn} 
$$
A[\psi(x)] = \sum_i F_i(x,t) \cdot \di_{x_i} \psi + \sum_{i,j} D_{ij}(x,t)
\di^2_{x_i x_j}\psi $$
\end{lemma}


\subsection{Stopping Times}
\begin{defn}
Let $\Fil(t)$ be some filtration, a random variable $\t$ is called a
\emph{stopping time} if 
$$
\{\o : \t(\o) \leq t\} \in \Fil(t) \, \forall t  
$$
\end{defn}
The colloquial way of describing stopping times is at any time we know
whether $\t$ has occurred or not. For a counterexample, the time that a Wiener
Process achieves its maximum over some interval is not a stopping time, since at any
given time, we do not know if the maximum has occurred or not. The most common
example of a stopping time is the first hitting-time, which is the first time
$X_t$ leaves or enters some set. 
\begin{thm}[First-Hitting time] Let $E \subset \R^n, E \neq
\phi$ be open or closed
 
then $$\t := \inf\{ t \geq 0 | X_t \in E\}$$ is a stopping time.
\end{thm}
 
The reason stopping times are very useful is that all the facts so far quoted
for Ito calculus using integrals $\int_0^T \intd{W}$ remain true if $T$ is
replaced by a stopping time $\t$.
 
Also it allows us to link SDE's and PDEs using the generator, $A$, of the
diffusion: 
\begin{thm}[Dynkin's formula] Given $\t$ a stopping time, $\Exp[\t] < \infty$.
Then:
$$
\Exp[u(X_\t, \t)] =
u(x, 0) + \Exp\left[\int_0^\t \di_t u + A[u] \intd{s}
\right]
 $$
\end{thm}
This provides a link between PDEs and stochastic processes and allows us to go
back and forth in that we can find probabilistic results by
solving a PDE or we can approximate a PDE by simulating a stochastic process
and averaging.

\begin{ex}[\cite{Evansb} pg 99 - Expected hitting time to a boundary]
\label{ex:mean_hitting_time}
 Let $\O
\subset \R^n$ be a bounded open set with smooth boundary $\di \O$ then it is a
basic fact from PDEs theory that
\begin{equation}
\begin{cases}
-\frac{1}{2} \grad^2 u = 1  & \text{over } \O
\\
u =  0 &\text{on } \di \O
\end{cases}
\end{equation}
has a unique $C^\infty(\O)$ solution.

Let $X = W_t + x$ for any $x \in \O$ and define
 $$\t_x := \text{first time } X \text{ hits } \di \O$$
then the generator of $X$ is $A[\psi] = -\grad^2(\psi)/2$ and we will
have:

\begin{align*}
\Exp[u(X_\t)] - \Exp[ u(x_0)] =& \Exp \left[ -\frac{1}{2}\int_0^\t \grad^2
u\right]
\\
=& \Exp \left[ - \int_0^\t 1
\right]
\\
=-&\Exp [\t]
\end{align*}
Finally, invoke $u$'s BCs, $u|_{\di E} = 0$, and $X$'s ICs, $X_0=0 = x$ to
conclude that $$ u(x) = \Exp [\t]$$
The solution to the PDE evaluated at $x$ is the expected exit time from
$E$ for an $X_t$  starting at $x$.
\end{ex}

\section{Parameter Estimation for SDEs}
\label{sec:estimation}
Discuss MAx Likelihood, Fisher information, Fortet equation. 

Parameter estimation for SDEs has a rich body of theory, \cite{??} and there are
many mathematical techniques available. Let us rewrite the SDE
in \cref{eq:generic_Ito_SDE_Rn} in order to explicitly that the 
functions $F, G$ are parametrized by the some parameter set, $\th$.
\begin{equation}
dX_t = F(X,t;\th) dt + G(X,t;\th) dW,
\label{eq:generic_Ito_SDE_Rn_parameterized}
\end{equation}
For example in the case of the O-U process, \cref{eq:OU_equation_generic},
where $F(X,t;\th) = (\m - {X_t})/{\tc}$ and $G(X,t;\th) = \b$, the
parameter set is $\th = \{\m, \tc, \b\}$.

In the standard problem formulation of SDE parameter formulation, one has exact
observations, $\{x_n\}_{n=0}^N$ at times $\{t_n\}_{n=0}^N$ from a process $X_t$
satisfying \cref{eq:generic_Ito_SDE_Rn_parameterized} and one seeks to find the
values of the parameter set $\th$.

\subsection{Maximum Likelihood Estimation}
A fundamental method, both
practically and theoretically, for estimating parameters in an SDE is the {\sl Maximum
Likelihoood} (ML) method, which proceeds by seeking those parameters which
maximize the likelihood of the observed data, $\{x_n\}_{n=0}^N$. In particular
let $$L(\{x_n\}; \th) = \Prob[ X_0 = x_0\ldots X_n = x_n\ldots X_N = x_N |
\th]$$ be the joint probability of observing the data $\{x_n\}$ given the
parameter set $\th$, also known as the likelihood. Then the ML method seeks to
maximize $L$.

In the case of independent observations, the
likelihood is just the product of the individual probabilities of each observation. In the case of SDEs is only
slightly more complicated, due to the Markov nature of the stochastic process. In particular,
the likelihood becomes the product of the transition probabilities. Recall that
earlier we defined the transition probability, $\f(x,t| y,s)$ in
\cref{eq:transition_prob_defn}. We shall also this as $\f_\th(x,t|
y,s)$ is we need to explicitly be reminded of $\f$'s dependence on the
parameter set, $\th$. With that we the likelihood of the observed $x_n$
becomes
\begin{equation}
L(\{x_n\}; \th) = \prod_{n=1}^{N} f_\th(x_n, t_n| x_{n-1}, t_{n-1})
\label{eq:SDE_discrete_likelihood}
\end{equation}
Here, we assume that $x_0$ is fixed, otherwise we would have to add a term
specifying the probability distribution of $X_0$.

In general the transition density for a generic SDE is impossible to find
analytically. There are several ways to approximate it numerically. The most
generic relies on the numeric solution of the Fokker-Planck PDE in
\cref{eq:FP_pde}, but that is quite expensive and suffers from the
'curse-of-dimensionality' for SDEs of higher dimension. 

In some simple cases, the transition density {\sl can} be calculated. The OU
example is computed above is one such a case, where the transition density is
\begin{align*}
f(x_n, t_n| x_{n-1} t_{n-1}) &=
 f(x_n, \Delta| x_{n-1} 0)\\& =
 \frac{1}{ \b \sqrt{\tc 2\pi(1 -  e^{-2 \Delta/\tc}})}
 	\cdot \exp\left(\frac{\left( x - \mu)  - (x_{0} - \mu) \cdot
 	 e^{-\Delta/\tc} \right)^2  } {\t \s^2  (1-e^{-2 \Delta/\tc})}
 	\right) 
\end{align*}
assuming that $\Delta_n = t_n-t_{n-1} = \Delta$ is constant for all $n$.
With this one can form the likelihood and solve analytically for the
maximizers, $\{\hat\m , \hat\tc, \hat\b \}$.
\begin{eqnarray} 
\hat{ \mu} &=& 
\frac{  \sum_{n=1}^{N } 
     \left( X_n - e^{-\frac{\Delta} {\hat \tc}} X_{n-1} \right)} 
	 { N( 1-e^{-\frac {\Delta} {\hat \tc}}) }
\\
e^{-\frac {\Delta}{\hat{\tc}} } &=& 
\frac { \sum_{n=1}^{N} 
			( X_n -  \hat \mu)(X_{n-1} -  \hat \mu) }
    {   \sum_{n=1}^{N } \left( X_{n-1} - \hat \mu
    \right)^2 }
\\
\hat\beta^2 &=&  
\frac{ 2  \sum_{n=1}^{N_k}  \left( X_n - \hat \mu - (X_{n-1} -
\hat \mu) e^{-\frac {\Delta} {\hat \tc}} \right)^2 } 
	  { N (1-e^{-2\frac {\Delta} {\hat \tc}}) \hat \tc}
\end{eqnarray}
The solution for the ML estimates is almost explicit. It requires one numerical
single-dimensional root-finding, which is an easy numerical task.

\subsection{Numerical Simulation of SDEs}
TODO: Euler-Maruyama scheme nothing fancy here. \cite{Higham2001}.


\section{Deterministic Optimal Control}
\label{sec:deterministic_control}
Optimal Control theory has three main components - a state, $x$, a control, $u$
and an objective $J$ which is a functional of $x,u$ and which we try to either
minimize or maximize. Here, we will be minimizing.

We introduce the theory for a finite dimensional state, $x(t) \in \R^{n_x}$, but
we will not address state constraints. Our control will also be
finite-dimensional $u(t) \in \R^{n_u}$ and it will be allowed to take values in
some closed domain, $\Udomain$, of allowed controls, e.g.\ $\Udomain = \{ u_t
\in \R^{n_u} \, \st |u| \leq \Umax \}$ for a control constrained to a sphere.
The objective $J$ will be given by a time-integral-plus-terminal cost:
\begin{equation}
J[u] = \int_0^\tf L(x_s, u_s) \intd{s} + M(x_\tf)
\label{eq:generic_objective_function_deterministic}
\end{equation}
Where $\tf$ could be either a variable or fixed. 

The dynamics of $x$ are given by some controlled ordinary differential
equation (ODE) given some initial conditions:
\begin{equation}
\dot{x} = f(x,t); \quad x(0) = x_0
\label{eq:generic_dynamics_deterministic}
\end{equation}

Then we seek the optimal control $u^*$, such that:
\begin{equation}
u^* = \argmin_{u \in \Udomain} J[u]  
\label{eq:generic_objective_argmin}
\end{equation}

Note that $x$ may also have final conditions, which we will deal with later,
and/or path constraints. If there are path-constraints, we talk about a
state-constrained problem. However, we will not face path constraints in the
sequel and so will not say more about how to handle them.

There are essentially three methods for solving equation
\cref{eq:generic_objective_function_deterministic}. Two, Pontryagin's Minimum
Principle and Dynamic Programing, are analytic in nature providing a set of
equations that characterize the minimum and then solving these equations,
numerically if necessary. The third, the direct method, simply discretizes the
problem and solves the resulting nonlinear programing (NLP) problem via standard
NLP techniques. We will describe both Pontryagin's Minimum Principle and Dynamic
Programing below and indeed we will use both later on.

\subsection{Pontryagin's Maximum Principle}
Pontryagin's Principle is derived from on a variational argument, similar to the
Euler-Lagrange equations from the Calculus of Variations, and it characterizes
the optimal trajectory/control pair. It can be thought of as a generalization of
the zero-tangent rule (Fermat's Rule) for finding optima in single-variable
calculus and like both the Euler-Lagrange equations and Fermat's Rule it
provides necessary, but not sufficient conditions for a minimum.

Pontryagin's Principle also holds a similarity to Hamiltonian Mechanics, which
should not be surprising, since Hamiltonian Mechanics is founded on
the minimization of action.

There are several versions of Pontryagin's Principle, depending on whether $x$ has
terminal conditions or not, whether the final time, $\tf$ is specified or a
variable in itself and most importantly whether the state has constraints or
not. We will state the versions that are relevant for our needs, namely, $x$ has
no constraints and $\tf$ is specified.

We first introduce the Pontryagin Hamiltonian, $\H$, also called the control
theory Hamiltonian and an adjoint state, $p \in \R^{n_x}$:
\begin{defn} [$\H$] The control theory Hamiltonian is the function:
$$
\H(x,p, u) := f(x,u) \cdot p + L(x,u) \quad (x,p \in R^{n_x}, u \in \Udomain)
$$ 
\end{defn}

\begin{thm}[Pontryagin's Fixed-Time, Free-End-Point] Assume $u^*(t)$ is
optimal for
\cref{eq:generic_objective_function_deterministic,eq:generic_dynamics_deterministic}
and that $x^*$ is the corresponding trajectory. 

Then there exists a function $p^*:[0,\tf]:\ra \R^{n_x}$ st.
\begin{equation}
\begin{cases}
\dot{x}^*(t) &=  \grad_p \H(x^*,p^*, u^*) = f(x^*, u^*)
\\
x(0) &= x_0
\end{cases}
\label{eq:pontryagin_state_ode}
\end{equation}
\begin{equation}
\begin{cases}
\dot{p}^*(t) &= -\grad_x \H(x^*,p^*, u^*) = -\grad_xL - p \grad_x f
\\
p^*(\tf) &= \grad_x M(x)
\end{cases}
\label{eq:pontryagin_adjoint_ode} 
\end{equation}
and
\begin{equation}
\H(x^*(t), p^*(t), u^*(t)) = \min_{u(t) \in \Udomain(t)}  \H(x^*(t), p^*(t),
u(t))
\label{eq:pontryaginH_optimality_condition} 
\end{equation}
\end{thm}
We will not give a proof of this theorem (see A.2 in \cite{Evansb}),
but we do provide a heuristic derivation in 
\cref{sec:Pontryagin_heuristic_derivation}, which is very helpful in deriving
the necessary conditions for PDE dynamics and it explains the otherwise
mysterious origins of the optimal control Hamiltonian, $\H$, and the adjoint
state, $p$.

At each time, $t$, we obtain the optimal control $u(t)$, by solving
\cref{eq:pontryaginH_optimality_condition}. This can be easy, hard or useless.
In the easy case, the minimum is analytically obvious, e.g.\ if the Hamiltonian
is quadratic in $u$. If it is linear in $u$ then the minimum will occur on the
boundaries of the control feasible region and we talk about a bang-bang
control. Conversely, solving the minimum of $\H$ with respect to $u$ may be impossible,
since $x,p$ are unknown. In this case we need to have an iterative scheme which
alternately solves
\cref{eq:pontryaginH_optimality_condition} 
and then \cref{eq:pontryagin_state_ode,eq:pontryagin_adjoint_ode}. Basic
numerical techniques using this idea are discussed in the appendix in
\cite{Kirk2004}. However, if we are going to be solving the problem numerically,
then the direct method might be superior, see \cite{Ross2005} for a discussion.
 
Finally, it is possible that \cref{eq:pontryaginH_optimality_condition} does not
depend on $u$ explicitly (it will still depend  on $u$ implicitly through $x,p$)
and so we cannot find $u$ at all by minimizing $\H$ wrt.\ $u$. This happens, for
example, if the cost, $L$, does not depend on $u$, the dynamics, $f$, are linear
in $u$ and $p=0$ over an interval of positive length. Such an optimal control
problem is called singular and it is discussed briefly in \cite{Kirk2004}, and
the theory can still be useful in finding a solution, but many complications can
and do arise. We face a singular control problem in our own work later, however
instead of tackling it head on, we merely regularize the problem - thus we will
not say much more on Singular Control Problems.

The version of Pontryagin's Principle for Fixed End Point, meaning $x$ has some
terminal conditions, is very similar to the one with free end-point. Suppose 
\begin{equation}
\dot{x} = f(x,t); 
\quad
\begin{cases}
x(0) = x_0
\\
x(\tf) = x_f
\end{cases}
\label{eq:generic_dynamics_deterministic_fixed_end_point}
\end{equation}

\begin{thm}[Pontryagin's Fixed-Time, Fixed-End-Point] Assume $u^*(t)$ is
optimal for
\cref{eq:generic_objective_function_deterministic,eq:generic_dynamics_deterministic_fixed_end_point}
and that $x^*$ is the corresponding trajectory. 

Then there exists a function $p^*:[0,\tf]:\ra \R^{n_x}$ st.
\begin{equation}
\begin{cases}
\dot{x}^*(t) &=  \grad_p \H(x^*,p^*, u^*) = f(x^*, u^*)
\\
x(0) &= x_0
\\
x(\tf) &= x_f
\end{cases}
\label{eq:pontryagin_state_ode}
\end{equation}
\begin{equation}
\dot{p}^*(t) &= -\grad_x \H(x^*,p^*, u^*) = -\grad_xL - p \grad_x f
\label{eq:pontryagin_adjoint_ode} 
\end{equation}
and
\begin{equation}
\H(x^*(t), p^*(t), u^*(t)) = \min_{u(t) \in \Udomain(t)}  \H(x^*(t), p^*(t),
u(t))
\label{eq:pontryaginH_optimality_condition} 
\end{equation}
\end{thm}
I.e. $p$ loses its final conditions because $x$ has them. This relates to the
variational nature of Pontryagin's Principle and the idea that $p$ represents
the sensitivity of the objective wrt. small changes in $x$. Thus, if $x$ is
fixed, there are no variations and $p$ can be anything.
% To illustrate Pontryagin's Principle we will solve the moon
% lander problem (Ex. 4.4.4 \cite{Evansb}), with the twist of holding the final
% time fixed. 
% \begin{ex}
% 
% \end{ex}

\subsection{Dynamic Programing - the Hamilton-Jacobi-Bellman equation} 
Dynamic Programing uses backwards recursion to tabulate the optimal control
starting from the terminal time. The basic object in dynamic programing is the value function, $v$. In order to
introduce it, we first extend our definition for the objective, $J$, to include later
times, with different initial conditions. 

The running cost-to-go corresponding to
\cref{eq:generic_objective_function_deterministic} is:
\begin{equation}
J[u; x, t] = \int_t^\tf L(x_s, u_s) \intd{s} + M(x_\tf), \quad x_t = x
\label{eq:generic_cost_to_go_deterministic} 
\end{equation}
So that $J[u; x_0, 0] = J[u]$. 

The dynamic programming value function, $v$ is then defined by:
$$
v(x,t) = \inf_{u \in \Udomain} J[u; x,t] 
$$
it is also called the optimal cost-to-go. 

We immediately note the terminal conditions on $v$:
\begin{equation}
v(x,\tf ) = M(x)
\end{equation}


The Dynamic Programing approach is then to solve for $v$ backwards and in the
process infer the control $u$.
\begin{thm}[Hamilton-Jacobi-Bellman Equation] Assume that $v$ is $C^{1,1}$ in
$x,t$. Then $v$ solves:
\begin{equation}
\begin{cases}
-\di_t v(x,t) &= \max_{u(t) \in \Udomain(t)} \left\{ f(x,t) \cdot \di_x v +
L(x,u) \right\}
\\
v(x,\tf) &= M(x)
\end{cases}
\end{equation}
\end{thm}
We will not derive the HJB equation here, but we will derive a similar version
later for the stochastic case, when the dynamics are given by SDEs. 

% \subsection{Model Predictive Control}
% Solving the HJB equations and obtaining a feedback control is usually
% impossible. Thus a practical alternative is to solve a simplified problem on a
% limited horizon and use the simplified solution now and then after stepping
% forward in time and observing the system in a new position, to solve a revised
% limited horizon problem and use the new solution.
% Let us make this more precise.

\section{Stochastic Optimal Control}
\label{sec:stochastic_control}
Here we extend the Dynamic Programing theory to the case where the dynamics are
governed by an SDE:
\begin{equation}
dX = F(X,t, u) dt + G(X,t) dW
\end{equation} 
i.e.\ we assume control affects only the drift term. This 
assumption is not necessary, but it corresponds to our problems.

For stochastic dynamics one can only optimize probabilistically and so
$J$ becomes:
\begin{equation}
J[u] = \Exp \left[ \int_0^\tf L(x_s, u_s) \intd{s} + M(X_\tf)
\right] 
\label{eq:generic_objective_function_stochastic}
\end{equation}
while the running cost-to-go is:
\begin{equation}
J[u; x, t] = \Exp \left[ \int_t^\tf L(X_s, u_s) \intd{s} + M(X_\tf) \right], 
\quad X_t = x
\label{eq:generic_cost_to_go_stochastic}
\end{equation}
the value function is:
\begin{equation}
v(x,t) := \inf_{u \in \Udomain} J[u; x,t]
\end{equation}

Finally, we again have that $v$ satisfies a certain PDE:
\begin{thm}[Stochastic HJB] 
\label{thm:stochastic_hjb}
\begin{equation}
\begin{cases}
-\di_t v(x,t) &=  \max_{u(t) \in \Udomain(t)} \big\{ L(x,u)  +
F(x,t,u) \cdot \di_x v
\big\} + \frac{G^2(x,t)}{2} \di_x^2v
\\
v(x,\tf) &= M(x)
\end{cases}
\end{equation}
\begin{proof}[Heuristic Derivation (from \cite{Evansb})] Suppose we are at time
$t$ and $X_t = x$.

Take a time increment $[t, t+h]$ and assume that during that time we apply a
control $u$ and subsequently we apply the the optimal control $u^*$. The
running-cost will then break down as: 
$$
J[u; x,t] = \Exp \left[ \int_t^{t+h} L(X_s, u_s) \intd{s}  + v(X_{t+h},
t+h) \right] $$

Since $v(x,t) = \inf J(u; x,t)$, we must have that:
$$
v(x,t) \leq  \Exp \left[ \int_t^{t+h} L(X_s, u_s) \intd{s}  + v(X_{t+h},
t+h) \right] $$
or, rearranging, that
\begin{align*}
0 \leq^&  \Exp \left[ \int_t^{t+h} L(X_s, u_s) \intd{s} \right]  
+ \underbrace{\Exp \left[ v(X_{t+h}, t+h) - v(x,t) \right]}_{\Exp\left[ dv
\right]}
\end{align*}
Now, $\Exp\left[ dv \right]$ can be expressed using Dynkin's Formula, as:
$$
\Exp[dv] = \int_t^{t+h} \di_t v +  F\cdot \di_x v + \frac{G^2 }{2}\cdot \di_x^2
v \intd{s} $$
Plugging that back, we get
$$
0 \leq \int_t^{t+h} L(X,u) +  \di_tv +  F\cdot \di_x v + \frac{G^2 }{2}\cdot
\di_x^2 v \intd{s} $$
Taking $h \ra 0$ we get 
$$
0 \leq  L(x,u) +  \di_tv(x,t) +  F\cdot \di_x v(x,t) + \frac{G^2 }{2}\cdot
\di_x^2 v(x,t) $$
and we conjecture that for the actual optimal control, the inequality becomes an
equality:
\begin{equation}
\begin{cases}
0 &=   L(x,u^*)+ \di_t v(x,t) +
 F(x,t,u^*)\cdot \di_x v(x,t) + \frac{G^2(x,t)}{2}\cdot \di_x^2 v(x,t)
\\
u^*(t) &= \argmax_{u \in \Udomain(t)}  
\big\{L(x,u) + F(x,t,u)\cdot \di_x v \big\}
\end{cases}
\end{equation}
\end{proof}
\end{thm}
% A rigorous proof can be found in \cite{Krylov2008}, sec???ch???

\section{Deterministic Optimal Control in Infinite Dimensions}
Now we discuss Optimal Control when the dynamics are deterministic but of
infinite-dimension, i.e. the state evolution is a PDE instead of an ODE.  The
reason we discuss this, is that optimization of ordinary SDEs in an open-loop
control is actually a deterministic optimal control problem in infinite
dimensions.

Why?

Since the objective is probabilistic and the probability density evolution is
given by the (deterministic) Fokker-Planck PDE, we need to optimize a
deterministic system whose dynamics are given by the Fokker-Planck equation.

There is no all-encompassing generalization of Pontryagin's Principle to
infinite dimensions, but a Maximum Principle can be derived for certain systems.
To illustrate the idea we will discuss a simple case as in \cite{Palmer2011}.
Our controlled SDE will be given by: 
$$ dX_t = F(X,t, u) dt + G(X,t) dW, $$
The corresponding probability density is given by: $$ \di_t \f=
-\sum_i \di_{x_i} \left[ F_i(x,t,u) \f(x,t) \right] + \sum_{i,j}  \di^2_{x_i
x_j} \left[ D_{ij}(x,t) \f(x,t) \right] $$ which we will write as $$ \dot{\f} =
\L_{u} [\f] $$ where $\L_{u}$ is the differential operator corresponding to the
RHS of the Fokker-Planck equation parametrized by the control $u$. For now we
will assume that there are no BCs, and the domain of $X$ is all of $\R^n$.

A natural inner-product, $ \langle.,. \rangle$ on functions in $L^2[\R^n]$ is
given by: $$
 \langle\psi,\phi  \rangle = \int_{\R^n} \psi(x) \phi(x) \intd{x}
$$
Our objective will be to minimize:
\begin{equation}
J[u] = \int_0^\tf  \langle L(\cdot,s,u), \f_u(s) \rangle \intd{s} +  
\langle M(\cdot),\f(\tf) \rangle
\label{eq:generic_objective_function_probabilistic}  
\end{equation}.

\Cref{eq:generic_objective_function_probabilistic} is a very natural cost
function in the stochastic dynamics case, because it can also be written as: 
$$
J[u] = \Exp \left[ \int_0^\tf  L(X_s, s,u) \intd{s} + M(X_\tf) \right] 
$$ which
is just \cref{eq:generic_objective_function_stochastic}. We use
\cref{eq:generic_objective_function_stochastic} if we have some feedback and
want to use dynamic programming. We use
\cref{eq:generic_objective_function_probabilistic} if we have no observations
and need to run an open-loop control.

Since, \cref{eq:generic_objective_function_probabilistic} is the objective
function for deterministic dynamics, we can use Pontryagin's Principle adapted
to the case of infinite dimensional dynamics (PDEs). 

Introduce an adjoint co-state, $\l \in \R^n$ and let the Hamiltonian, $\H$ be:
$$
\H(\f, \l, u)= \langle \phi, L + \Lstar[\l] \rangle$$
where $\Lstar$ is the adjoint operator to $\L$ defined via:
$$
\langle \L [\phi], \l \rangle =\langle \phi, \Lstar[\l] \rangle
$$

Let $\l$ evolve (backwards) according to:
\begin{equation*}
\begin{cases}
-\di_t \l =& \Lstar[\l] + L
\\
\l(\tf) =& M(x)  
\end{cases}
\end{equation*}
then the optimal control, $u$, is given via
$$
u = \argmin_u {\H(\f, \l, u)}
$$   

An important fact, useful for computations, is that the total derivative of the
objective with respect to the control is given by the partial derivative of the
Hamiltonian with respect to the control:
\begin{equation}
\grad_u \big[J[u] \big] = \di_u \H(\f, \l, u)
\label{eq:objective_gradient_wrt_control}
\end{equation}

So for a given $u$, we can compute $\f,\l$, calculate the objective gradient
using \cref{eq:objective_gradient_wrt_control} and then iterate in the direction
of descent (if we are minimizing).

All that remains is to compute $\Lstar$. It turns out that if we do not have any
BCs, then
$$
\Lstar[\l] = \sum_i F_i(x,t) \cdot \di_{x_i} \l + \sum_{i,j} D_{ij}(x,t)
\di^2_{x_i x_j}\l 
$$ 
which is just the generator of $X$! 

We will see later, that things become more complicated when we need to
consider a PDE's BCs.

 
\subsection{Heuristic Derivation of the Optimality Conditions for PDEs}
\label{sec:Pontryagin_heuristic_derivation}
TODO: \cite{Borzi2012,Lenhart2007}

\section{Optimal Design}
\label{sec:optimal_design}
Optimal design is approach to design of statistical experiments that is guided
by the 
desire to optimize some formal measure of the parameter estimates of the
experiments. TODO: cite Pukelheim. Common criteria are minimizing the
determinant or the trace of the covariance matrix of the parameter estimators. 
The covariance matrix, also known as the Fisher Information, however often
depends on the very parameters that one seeks to estimate. 

An alternative to the Fisher Information as an
objective for the formal design of experiments is to use of concepts from
Information Theory,  \cite{MacKay2003}. 

We now define and discuss the {\sl Mutual Information} between
two random variables $X, \Th$. For reference we will follow
\cite{MacKay2003}. 

\begin{defn}Mutual Information:
Given two random variables $X,\Th$ with joint probability density
$p(x,\th)$ and marginal densities $p(x), p(\th)$, the Mutual Information between
$X$ and $\Th$ is given by
\begin{equation}
I(X,\Theta) = \int_\Theta \int_X p(x,\th) \cdot \log \left(
\frac{p(x,\th)}{p(x)p(\th)}\right) \intd{x} \intd{\th}
\label{eq:mutual_info_defn}
\end{equation}
\end{defn}

It is obvious that if $X,\Th$ are independent than $I(X,\Theta) = 0$ and it can
be verified that $I\geq0$ and that it is maximized if $\Theta$ is a function of
$X$; that is, if the entropy of $\Th$ conditional on $X$ is zero. 
The mutual information, $I(X,\Th)$ represents the 'average
reduction in uncertainty about $\Th$ that results from learning the value of
$X$' \cite{MacKay2003}. This statement is formally correct if one takes
 'uncertainty' to mean the entropy of a random
 variable.
 
In order to make use of the Mutual Information in a parameter estimation
context, we need to specify a prior belief distribution over the  parameters.  
$$
\rho(\th) = \Prob(\Theta = \th)
$$
A prior distribution is standard and central concept in Bayesian statistics.
 
With that we are treating the unknown parameter, $\th$ like a random variable,
which we write as $\Th$. It is then natural to seek an experiment which
maximizes the Mutual Information between, $\Th$ and the observations $X$

To obtain the joint and margincal distributions of $\Th$ and $X$, we
need to recall Bayes' formula which relates the posterior of the parameters,
$p(\th|x)$ to the observations and the parameters' prior. 

\begin{equation}
p(\th|x) = \frac{L(x|\th)\rho(\th)}{\int_\Th L(x|\th)\rho(\th)}
\label{eq:bayes_formula}
\end{equation}
where $L$ is the likelihood of the observations.

Moreover, the marginal distribution of the parameters, is just its prior,
$p(\th) = \rho(\th)$, while the joint distribution of the parameters and the
observations is $$p(x,y) = L(x|\th)\rho(\th)$$.

while the $x$ marginal is $$p(x) = \int_\Theta L(x|\th)\rho(\th) \intd{\th}$$
Plugging the three expressions for $p(x,\th), p(x)$ and $p(\th)$ in the
definition, \cref{eq:mutual_info_defn}, gives:
\begin{equation}
I(X,\Th) = \int_\Theta \int_X L(x|\th)\rho(\th) \cdot 
\log \left( \frac{L(x|\th) }
				{\int_\Theta L(x|\th)\rho(\th) \intd{\th}  } \right)
\intd{x}\intd{\th}.
\label{eq:mutual_info_posterior_vs_observations} 
\end{equation} 
\Cref{eq:mutual_info_posterior_vs_observations}  is used in
\cref{ch:optimal_design}.




\section{Mathematical Models in Neuroscience}
\label{sec:math_models_in_neuroscience}
We now describe in detail the basic mathematical model of a neuron that will be
used in the sequel. 

An introduction to mathematical models in neuroscience is given in Gerstner and
Kistler, \cite{Gerstner2002}, also available online, while the book Stochastic
Methods in Neuroscience, \cite{Laing2009} gives a nice overview of several
current research applications of stochastic techniques to neuroscience.


Neurons relay information by means of voltage spikes - sudden sharp increases in
voltage. Although many details remain unclear, the information content is
thought to be contained in the length of the time-interval between these spikes.
In the simplest case, this can be thought of as a rate - the average number of
spikes per time interval, but more complicated coding schemes are hypothesized to
exist. 

Many experiments allow for manipulating an individual neural cell. A natural
goal then is to make a cell produce a given spike train. This may arise, for
example, in brain-machine interfaces or in artificial prosthetics. 


TODO: Go from HH $\ra$ FN/ML  $\rightarrow$ LIF

Then add noise\ldots

Thus the final simplification of the basic spike-generation mechanism is the
noisy leaky-integrate-and-fire model: 
%TODO: tau_c -> tau
\begin{equation}
\begin{gathered}
dX_t = \left(\a(t) - \frac{(X_t - \m)}{\tc} \right) \intd{t} + \b \intd{W_t},
\\
X(0) = 0,
\\
X(\ts) = \xth \implies  
\begin{cases}
X(\ts^+) = 0 &  
\end{cases}
\end{gathered}
\label{eq:X_evolution_uo}
\end{equation}
That is $X$ follows an OU process, but upon reaching a pre-determined threshold,
$\xth$, a 'spike' is deemed to have occurred and the process is 'reset' to an
initial value, here $0$.

Alternatives to the hard-threshold integrate-and-fire model are so called
'soft-threshold' models which use a {\sl hazard} function which is akin to the
intensity of a Poisson Process to determine the spike time. The hazard function
is increasing in voltage, thus higher voltages imply higher likelihood of
spiking. The hard-threshold model can be seen as a special case of the
soft-threshold model, given the hazard function which is 0 below the threshold
and is infinite above the threshold. We will not address hazard-function based
spiking models in the rest of the thesis. 
