\chapter{Mathematical Background}
\label{ch:math_background}
\input{../OptEstimate/local_style.sty}
Here we collect a list of mathematical tools that are used in the thesis. We
first point the reader to the general references by Oksendal for SDEs
\cite{Oksendal2007} and by Fleming and Rishel for Optimal Control
\cite{Fleming1975}. Another very readable introduction to the field of both SDEs
and Optimal Control are the online notes of Professor L. Evans
\cite{Evansa,Evansb}. We have also used Jacobs as our main tutorial on
first-passage times for SDEs \cite{Jacobs}.
 
\section{Stochastic Differential Equations}
\label{sec:SDEs}
In view of our ultimate goals, we will restrict ourselves to  stochastic
processes whose sample paths are continuous, and more specifically to SDEs
driven by Brownian motion. We will not provide proofs of results in this
section, but only state the results with a view towards establishing the
notation for the rest of the thesis. We will assume that the reader is familiar
with the following concepts:
\begin{itemize} 
  \item a probability space $\{\O, \sAlg, P\}$, where $\O$ denotes the probability space, $\sAlg$ a sigma-algebra, and $P$ a probability measure
  \item a continuous-time stochastic process, $X_t$
  \item a filtration $\Fil_t$, in particular the filtration generated by a
  stochastic process.
\end{itemize}

\subsection{The Wiener Process and the It\^o Integral}

The Wiener Process is the fundamental building block of the stochastic calculus. 
It is often called Brownian Motion and we will denote it $\{W_t\}_{t\geq 0}$. 
\begin{defn}Wiener Process, $W_t$:
\begin{enumerate}
  \item $W_0 = 0$.
  \item $W_t - W_s \sim N(0, |t-s|)$ , i.e. the process increments are
  normally distributed with mean 0 and variance $|t-s|$.
  \item $\forall \{t_i\}_1^N, \quad \{W_{t_i} - W_{t_{i-1}} \}_2^N \sim$
  independent, i.e.\ the process has independent increments.
\end{enumerate}
\end{defn}
The fact that the finite incremental distributions suffice to specify a unique
continuous-time stochastic process is known as Kolmogorov's extension theorem.
Now, we collect a few more relevant properties of the process $W_t$ and its
sample paths:
\begin{enumerate}
  \item The sample paths of $W_t$
are almost surely (a.s.) continuous and are in fact Holder continuous for any
exponent $\g < 1/2$
\item The sample paths of $W_t$ are nowhere differentiable
\item $W_t$ is a Markov process: $\Prob[W_t \in B | \s(W_{s' \leq s })] =
\Prob[W_t \in B \,| \,W_s]$, for any Borel set, $B \subset \R$, where $\s(W_{s'
\leq s })$ is the filtration generated by the $W_t$.
\end{enumerate}

We now turn to defining stochastic integrals based on the Wiener Process.
 
\begin{defn} Progressively Measurable Functions:

Let $\Fil_t$ be the filtration generated by the Wiener Process.

Let $X_t$ be a stochastic process which is $\Fil_t$-measurable $\forall t$ and
which is jointly measurable in $(t,\o)$. We call such an $X_t$
\emph{progressively measurable}.
\end{defn}

\begin{defn} $\Ltwopm, \Lonepm$

We define $\Ltwopm[0,T]$ as the space of all progressively measurable
$X$ such that
\begin{equation*}
\Exp \left[ \int_{[0,T]} X_t^2 \intd{t} \right] < \infty
\end{equation*}

Similarly, we define $\Lonepm[0,T]$ as the space of all progressively measurable
$X$ such that
\begin{equation*}
\Exp \left[ \int_{[0,T]} |X_t| \intd{t} \right] < \infty
\end{equation*} 
\end{defn}

$\Ltwopm$ will be the class of functions for which the It\^o integral is
well-defined. 
\begin{defn} It\^o Integral:
\label{defn:ito_integral}

Let $P^n := {a = t^n_1 \ldots t^n_{m_n} = b}$ be a partition of the interval
$[a,b] \subset [0, \infty)$. Let $|P_n| = \sup_i|t_i - t_{i-1}|$. 
Let $ \lim_{n \rightarrow \infty} |P_n| \rightarrow 0 $ and consider an $ X_t
\in \Ltwopm[a,b]$,

then
\begin{equation}
\int_{[a,b]} X_t \intd{W} := \lim_{n \rightarrow \infty}  
\sum_{i=1}^{m_n} X_{t_i}\left( W(t_{i+1}) - W(t_{i})\right)
\end{equation}

\end{defn}

To be precise, our definition is actually a theorem, and the real
definition is one that uses step functions and passes to the limit.  Also we
will write the limits of integration $\int_0^T \cdot  \intd{W}$ or $\int_{[0,T]}
\cdot  \intd{W}$ interchangeably. 

Again, we state without proof a few interesting properties of $\int X_t
\intd{W}$. \begin{thm} It\^o Integral Properties

\begin{enumerate}
  \item $\Exp[\int_0^T X_t \intd{W} ] = 0$ 
  \item $\Exp[\left(\int_0^T X_t \intd{W}\right)^2 ] = \Exp[\int_0^T X_t^2
  \intd{t}]$
  \item $I(t) = \int_0^t X_t \intd{W} $ is a martingale 
  \item $I(t) = \int_0^t X_t \intd{W} $ has continuous sample paths.
\end{enumerate}
\end{thm}

\subsection{It\^o SDEs and It\^o's Lemma}
\begin{defn}[It\^o SDE] Let
$B(x,t):\R\times [0,T]\ra \R$, $G(x,t):\R\times [0,T]\ra \R,$ be given
functions. We say that a stochastic process $X$ satisfies:
\begin{equation}
dX =B dt + G dW
\end{equation} 
over $[0,T]$ if
\begin{enumerate}
  \item $X$ is progressively measurable wrt. $\Fil_t$
  \item $B(X,t) \in \Lonepm[0,T]$
  \item $G(X,t) \in \Ltwopm[0,T]$ 
  \item $X_t = X_0 + \int_0^t B(X_s, s) \intd{s} + \int_0^t G(X_s, s) \intd{W_s}$.
\end{enumerate}
\end{defn}  

We are now ready to present the celebrated It\^o Lemma which is the chain-rule of
stochastic calculus.

\begin{thm}[It\^o Lemma]
Suppose $X$ satisfies the stochastic differential $dX_t =
B(X,t)dt + G(X,t) dW$ as above and take $v(x,t) \in C^{2,1}[ \R \times [0,T]]$,
that is $v$ is twice-continuously differentiable wrt.\ its first argument and
once wrt.\ its second argument.

Set $Y(t) = v(X,t)$
\\
then
$$
dY =  \left( \di_t v + \di_x v \cdot B + \di^2_x v \cdot
\frac{G^2}2 \right)
\intd{t} + \left(   \di_x v\cdot G  \right)\intd{W}
$$ 
\end{thm} 
 

As an example we will discuss the Ornstein-Uhlenbeck process, which is the
basis for the models we will face later on.
\begin{ex}[O-U Process] Let $X_t$ follow:
\begin{equation}
dX = \left( \frac {\m -X_t}{\tc} \right) dt + \b dW
\label{eq:OU_equation_generic}
\end{equation}
with an initial condition, $X_0$, which may be an arbitrary distribution
independent of the Wiener Process, $W$.
We can solve this as follows: 
\begin{align*}
dX =& \left( \frac {\m -X_t}{\tc} \right)  dt + \b dW
\\
dX + \frac {X_t}{\tc} dt=&  \frac\m\tc dt + \b dW
\\
e^{t/\tc} dX + e^{t/\tc}\frac {X_t}{\tc} dt
=& e^{t/\tc}\frac\m\tc dt + \b e^{t/\tc} dW
\\
Xe^{t/\tc} - X_0 
=& \int  e^{t/\tc}\frac\m\tc dt +  \int \b e^{t/\tc} dW
\\
X_t =& e^{-t/\tc} X_0 + \m(1-e^{-t/\tc}) +  \frac{\sqrt{\tc}\b
e^{-t/\tc}}{\sqrt{2}} W(e^{2t/\tc}-1)
\end{align*}
which means that $X$ forgets its initial conditions exponentially fast and
converges to a normal random variable with mean $\m$ and variance
$\tfrac{\tc \b^2}{2}$.
\end{ex}

At the end of this sub-section, we state It\^o's Lemma for a multidimensional
state.
 
% \begin{thm}[It\^o Lemma for $dW \in  \R^{m}$] Suppose $X$ satisfies the
% stochastic differential $dX_t = F dt + G dW$, where $ F \in \R, G \in
% \R^{1\times m}$ and $dW = \left(dW^{(i)}\right) \in  \R^{m}$ is a vector of
% independent Wiener Processes. Take $u(x,t) \in C^{2,1}[ \R \times [0,T]]$. Let
% $D :=\left[{\sum_k G_{k} G_{k} } \right] \in \R$. Set $Y(t) = u(X,t)$. Then $$
% dY =  \left( \di_t u +  \di_{x} u \cdot F + \di^2_{x} u \cdot D \right)
% \intd{t} + \left(  \di_{x} u  \cdot  G \right) dW $$ \end{thm}

\begin{thm}[It\^o Lemma for $X \in R^n$, $dW \in  \R^{m}$]
Suppose $X$ satisfies the stochastic differential $dX_t = B(X,t) dt + G(X,t)
dW$, where $B : [R^n \times [0,T] \ra  \R^n]$, $G: [R^n \times [0,T]\in
\R^{n\times m}$ and $dW = \left(dW^{(i)}\right) \in  \R^{ m}$ is a vector of independent Wiener Processes. Take $u(x,t) \in C^{2,1}[ \R^n
\times [0,T]]$. Let $D_{ij} := \tfrac{1}{2} \left[{\sum_k G_{ik} G_{jk} }
\right]_{ij}$. Set $Y(t) = u(X(t),t)$. Then
$$
dY =  \left( \di_t u + \sum_i \di_{x_i} u \cdot B_i + 
\sum_{i,j} \di^2_{x_i x_j} u \cdot D_{ij} \right)
\intd{t} +
 \left(  \sum_{ij} \di_{x_i} u  \cdot G_{ij} dW^{(j)} 
\right)$$
\label{thm:ito_lemma}
\end{thm} 

\subsection{Fokker-Planck and Kolmogorov's Backward Equations}
The Fokker-Planck equation and Kolmogorov's Backward equation describe the
forward (resp. backward)  evolution of the probability density of $X_t$.

For the rest of this section we will work in $ \R^n$, i.e. $X$ will be an $n-$dimensional vector that satisfies the stochastic differential
\begin{equation}
dX_t = B(X,t) dt + G(X,t) dW,
\label{eq:generic_Ito_SDE_Rn}
\end{equation}
using the notation defined in Theorem \ref{thm:ito_lemma}. 

Let 
\begin{equation}
 \f(x,t| y,s) \intd{x} =  \Prob[X_t \in \intd{x} | X_s = y]
 \label{eq:transition_prob_defn} 
 \end{equation}
be the transition probability density. Then $\f$ satisfies:
\begin{equation}
\di_t \f(x,t| y,s)= -\sum_i \di_{x_i} \left[ B_i(x,t) \f(x,t| y,s) \right] 
+ 
\sum_{i,j}  \di^2_{x_i x_j} \left[ D_{ij}(x,t) \f(x,t| y,s) \right].
\label{eq:generic_FokkerPlanck}
\end{equation} 
This is called the \emph{Fokker-Planck} or \emph{Forward Kolmogorov} equation.
It can be seen as an equation describing the conservation of probability. To this end define the
probability current $\phi \in \R^n$ and its $i$th component $\phi_i \in \R$
as:
\begin{equation}
\phi_i =   B_i(x,t) \f(x,t| y,s) 
- \sum_{j}  \di_{x_j} \left[ D_{ij}(x,t) \f(x,t| y,s)\right].
\label{eq:FokkerPlanck_prob_flux}
\end{equation}
Then the Fokker-Planck equation, \cref{eq:generic_FokkerPlanck}, can be written as:
\begin{equation}
\di_t \f= -\grad \cdot \phi,
\label{eq:Fokker_planck_as_prob_flux}
\end{equation}
which just says that the change in probability is the
difference between the flow in and the flow out. 

Initial conditions for $\f$ are given by the distribution of $X_s$. 
Boundary conditions (BCs) depend
on the domain of $X$ and what happens to $X$ once it hits its boundary. If the domain
is all of $\R^n$ then we only insist that $\lim_{|x| \ra \infty} \f = 0$. If the
domain has boundaries, then there are two common scenarios which we will also
encounter in the sequel:
\begin{enumerate}  
  \item absorbing BCs
  \item reflecting BCs.
\end{enumerate}
At an absorbing boundary, the particle $X$ is removed and $\f=0$ there.
In this situation, we will not have conservation of probability and the integral
of $\f$ over $X$'s domain will monotonically decrease.

At a reflecting boundary, the particle $X$ bounces back into its domain and we
will have that the probability current $\p\cdot n = 0$ where $n$ is the outward
normal at the reflecting boundary.

Now consider $\f$ as a function of the initial values $y,s$, holding the
terminal values $x,t$ fixed. Then
\begin{equation}
-\di_s \f(x,t|y,s)= \sum_i  \left[B_i(y,s) \cdot \di_{y_i}\f(x,t|y,s) \right] 
+ \sum_{i,j}   \left[ D_{ij}(y,s) \di^2_{y_i y_j}\f(x,t|y,s) \right]
\label{eq:FP_backward_pde}
\end{equation} 
This is the \emph{Backward Kolmogorov} equation for $\f$. Note the minus sign
in front $\di_s \f$. A mnemonic for the signs of the Backward vs. Forward
equation is that as $t$ increases in the forward case, $\f$ diffuses and so $\di_t$ and $\di^2_{x}$
have the same sign; in the backward case, as $s$ increases, that is as $s$ approaches $t$, $\f$
anti-diffuses and the two partial derivatives have opposite signs. 
  
The differential operator on the right-hand side of
\cref{eq:FP_backward_pde} occurs often in the study of SDEs and has its own
name.
\begin{defn}[Generator of an SDE] The Generator $A$ of $X$ is defined by:
$$
A[\psi(x)] = \lim_{t \searrow 0^+} \frac{\Exp[\psi(X_t)]  - \psi(x)}{t} ;  \quad X_0 =
x \in \R^n
$$
\end{defn} 
\begin{lemma} For an It\^o SDE as in \cref{eq:generic_Ito_SDE_Rn} 
$$
A[\psi(x)] = \sum_i B_i(x,t) \cdot \di_{x_i} \psi + \sum_{i,j} D_{ij}(x,t)
\di^2_{x_i x_j}\psi $$
\end{lemma}


\subsection{First-Hitting Times}
Most of the problems in this thesis are related to {\sl first-hitting times}.

\begin{defn}
Let $\Fil(t)$ be some filtration, a random variable $\t$ is called a
\emph{stopping time} if 
$$
\{\o : T(\o) \leq t\} \in \Fil_t \, \forall t  
$$
\end{defn}
The colloquial way of describing stopping times is that at any time we know
whether $T$ has occurred or not. For a counterexample, the time that a Wiener
Process achieves its maximum over some interval is not a stopping time, since at any
given time, we do not know if the maximum has occurred or not. The most common
example of a stopping time is the first hitting-time, which is the first time
$X_t$ leaves or enters some set. 
\begin{thm}[First-Hitting time] Let $E \subset \R^n$ be open or closed and non-empty
 
then $$T := \inf\{ t \geq 0 | X_t \notin E\}$$ is a stopping time.
\end{thm}
 
The reason stopping times are very useful is that all the facts so far quoted
for It\^o calculus using integrals $\int_0^t \intd{W}$ remain true if the
fixed time $t$ is replaced by a stopping time $T$. Also it allows us to link
SDE's and PDEs using the generator, $A$, of the diffusion: 
\begin{thm}[Dynkin's formula] Given $T$ a stopping time, $\Exp[T] < \infty$.
Then:
$$
\Exp[u(X_T, T) | X_0 =x] =
u(x, 0) + \Exp\left[\int_0^T \di_t u(X_t, t) + A[u(X_t,t) | X_0=0] \intd{t}
\right].
 $$
\end{thm}
This provides a link between PDEs and stochastic processes and allows us to go
back and forth in that we can find probabilistic results by
solving a PDE or we can approximate a PDE by simulating a stochastic process
and averaging.

This thesis is primarily concerned with the analysis of first-hitting times.
Thus we will often refer to the probability of a first-hitting time, $T$, to
take the value $t$ as
$$g(t) := \frac{1}{\intd{t}} \Pr(T \in [t, t+\intd{t}) |X_0 = x_0).$$

There is a simple relation between the hitting-time density, $g$, and the
transition density of $X_t$. Let $T$ be the first-hitting time of $X$ to the
boundary $\di E$ of the closed domain $E\subset  \R^n$, i.e.\ $T$ is the first
time that $X_t$ leaves $E$.  
Then if $f$ solves the
Fokker-Planck equation for $X$, such that $f\equiv 0$ on $\R^n \setminus E$, and
if $\phi$ is its probability flux defined in \cref{eq:FokkerPlanck_prob_flux}, then
we will have that
\begin{equation}
g(t) = \int_{\di E} \phi \cdot n \intd{S},
\label{eq:hitting_time_g_in_terms_of_outflow}
\end{equation}
where $n$ is an outward normal to $E$, the integral is a surface integral. 

We give a quick derivation of \cref{eq:hitting_time_g_in_terms_of_outflow}. Let
$$G(t) = \Pr(T\leq t | X_0=x_0) = \int_0^t g(s) \intd{s}$$ be the cumulative
distribution corresponding to $g$. Then $G(t)$ is the probability
that at time $t$ the particle has left the domain and so 
$$G(t) = 1- \int_E f(x,t|x_0,0) \intd{x}$$

Since $g(t) = \di_t G(t)$, we then have that  
$$g(t) = - \di_t \int_E f(x,t|x_0,0) \intd{x} = \int_E \grad \cdot
\phi(x,t|x_0,0) \intd{x} = \int_{\di E} \phi \cdot n \intd{S}.$$
where the second equality comes from the probability form of the Fokker-Planck equation,
\cref{eq:Fokker_planck_as_prob_flux}, and the third  equality is the divergence
(Gauss') theorem.

In the simplest case, of working in 1-D, such that $E = (-\infty, \xth)$ and
$\xth>x_0$, we will have 
$$  g(t) = \phi(\xth, t) - \phi(-\infty,t) = \phi(\xth, t),$$
since, at the lower boundary the probability flux $\phi(-\infty,t)=0$. Using
the expression for $\phi$ from \cref{eq:FokkerPlanck_prob_flux}, we arrive at 
 $$  g(t) = B(\xth,t)f(\xth,t ) -\di_x[ D(\xth,t) \cdot f(\xth,t )]= -\di_x[
 D(\xth,t) \cdot f(\xth,t )].$$
 Since, $f(\xth,t)=0$. In neural applications this is the form of $g$, that we
 will encounter. 

% \begin{ex}[\cite{Evansb} pg 99 - Expected hitting time to a boundary]
% \label{ex:mean_hitting_time}
%  Let $\O
% \subset \R^n$ be a bounded open set with smooth boundary $\di \O$ then it is a
% basic fact from PDEs theory that
% \begin{equation}
% \begin{cases}
% -\frac{1}{2} \grad^2 u = 1  & \text{over } \O
% \\
% u =  0 &\text{on } \di \O
% \end{cases}
% \end{equation}
% has a unique $C^\infty(\O)$ solution.
% 
% Let $X = W_t + x$ for any $x \in \O$ and define
%  $$\t_x := \text{first time } X \text{ hits } \di \O$$
% then the generator of $X$ is $A[\psi] = -\grad^2(\psi)/2$ and we will
% have:
% 
% \begin{align*}
% \Exp[u(X_\t)] - \Exp[ u(x_0)] =&
%  \Exp \left[ -\frac{1}{2}\int_0^\t \grad_x^2 u(X_t) \intd{t} \right]
% \\
% =& \Exp \left[ - \int_0^\t 1  \intd{t} 
% \right]
% \\
% =-&\Exp [\t]
% \end{align*}
% Finally, invoke $u$'s BCs, $u|_{\di E} = 0$, and $X$'s ICs, $X_0=0 = x$ to
% conclude that $$ u(x) = \Exp [\t]$$
% The solution to the PDE evaluated at $x$ is the expected exit time from
% $E$ for an $X_t$  starting at $x$.
% \end{ex}


\subsection{Numerical Simulation of SDEs}
In general SDEs, such as \cref{eq:generic_Ito_SDE_Rn}, cannot be solved
analytically and if one wants to obtain approximate paths from their solution,
one needs numerical methods. There are many numerical methods for approximating
an SDE as in \cref{eq:generic_Ito_SDE_Rn}, see e.g. \cite{Higham2001} for a
popular introduction. Here we will only describe the most basic method, the
Euler-Maruyama scheme, which will suffice for our purposes. In the
Euler-Maruyama scheme, the approximate solution to \cref{eq:generic_Ito_SDE_Rn}
is computed at predetermined time-nodes, $\{t_k\}_{k=0}^N$ with time intervals
$\Delta t_k = t_{k+1} - t_{k }$, which are often assumed to be constant, i.e.
$\Delta t_k = \Delta t\,\,\, \forall k$. Then given an initial condition $X_0 =
X(t_0)$, the approximate path, $\{X_k\}_{k=0}^N = \{X(t_k)\}_{k=0}^N$ is
obtained iteratively via:

\begin{equation}
X_{k+1} = B(X_k,t_k)  \Delta t_k   + G(X_k,t_k) \xi_k \sqrt{\Delta t_k }
\label{eq:euler_maruyama_discretization_generic_Ito_SDE}
\end{equation}
where $\xi_k$ is an independent draw from the standard normal distribution. 


\section{Mathematical Models in Neuroscience}
\label{sec:math_models_in_neuroscience}
We now describe in detail the basic mathematical model of a neuron that will be
used throughout most of this thesis. It is just a basic linear SDE together with an associated
first-hitting time specification.

An introduction to mathematical models in neuroscience is given in Gerstner et
al., \cite{Gerstner2014}, also available online. The book Stochastic Methods in
Neuroscience \cite{Laing2009} provides a nice overview of several current research
applications of stochastic techniques to neuroscience.

A neuron's most important property is its membrane potential - the electric
potential difference between the cell's interior and its surroundings. Neurons
relay information by means of voltage spikes - sudden sharp increases in their
membrane potential, which are then transmitted to other neurons that are
chemically or electrically connected to the spiking neuron. The information
content is thought to be contained in the timing of the spike, or equivalently
in the length of the time-interval between subsequent spikes. In the simplest
case, this can be thought of as a {\sl firing rate}, i.e. the average number of
spikes over some time interval, but more complicated coding schemes have been hypothesized
to exist. A spike in a given neuron can be triggered as a result of spikes coming from neurons 
connected to it. Special neurons called sensory neurons interface with external physical stimuli
 such as mechanical vibrations in the case of auditory neurons and light photons in the case of visual neurons in the retina.  

Given a small positive stimulus, the voltage of a neuron increases linearly, but
then relaxes back down to its equilibrium, pre-stimulus, level. However, if the
stimulus is sufficiently strong or if there are many small stimuli in a
sufficiently short time interval, the voltage will then go through a stereotypical large
 non-linear excursion before resetting back down to its equilibrium
 - this is a spike, which can propagate along the axon to other neurons. 
 The physical underpinnings of this event involves ion
 channels with different time-scales, with an initial fast excitation due to sodium ions (known as a depolarization) 
 followed by a slightly slower counter-acting inhibition by potassium ions (repolarization). 
 The full spike unfolds on the order of 1-5 milliseconds. 
 
 The first successful mathematical model describing this
 phenomena is the famous Hodgkin-Huxley (HH) model, which forms a system of 4
 ODEs, one for the membrane voltage and three for the dynamics of the ion
 channels. From a practical point of view however, the HH model of
neuron dynamics is quite complicated if one is only interested in spike times,
 rather than the detailed contributions of ion channels to the voltage. Thus there have been several
 reductions to this model. The standard approach is to keep the voltage equation, 
 as well as one more equation as a 'recovery' variable which works on a slower
 scale than the voltage; together these two variables produce the very sharp up-and-down voltage
 excursions. Two popular examples of such reductions are the Fitzhugh-Nagumo
 model and the Morris-Lecar model. The Morris-Lecar model
 is a 2-dimensional ODE for, respectively, the voltage and recovery variables $v(t), w(t)$. It reads:
\begin{equation}
\left\{
\begin{array}{ccl}
\dot{v}(t)  &=& \frac{1}{C}\Big(-g_{Ca}m_\infty(v) (v-V_{C_a}) -
g_K w (v-V_K) \\ && 
-g_L(v-V_L)+I(t)  \Big) \\
\dot{w}(t)&=& \alpha(v)(1-w) - \beta(w)w
\end{array}
\right.
\label{eq:ML_original_deterministic}
\end{equation}
where the auxiliary channel gating functions, $m_\infty, \alpha, \beta$ are given by:
\begin{eqnarray*}
m_\infty(v)&=&\frac{1}{2}\left(1+\tanh\left(\frac{v-V_1}{V_2}\right)\right),\\
\alpha(v) &=& \frac{1}{2}\phi \cosh\left(\frac{v-V_3}{2V_4}\right)\left(1+\tanh\left(\frac{v-V_3}{V_4}\right)\right),\\
\beta(v) &=& \frac{1}{2}\phi \cosh\left(\frac{v-V_3}{2V_4}\right)\left(1-\tanh\left(\frac{v-V_3}{V_4}\right)\right).
\end{eqnarray*} 
The term $I(t)$ represents current applied on the neuron, either from natural
external stimulation, an experimental control, or synaptic or electrical inputs from other neurons. 
In \cref{sec:morris_lecar_control}, we describe how the Morris-Lecar
deterministic ODE of \cref{eq:ML_original_deterministic} is extended to an SDE, and we demonstrate how its spikes times can be optimally controlled in spite of the noise.
 
The 2-dimensional models like Morris-Lecar are much easier to work with
mathematically and experimentally, for example for parameter estimation, but
they still require a lot of effort for describing the details of a neural spike.
If one is only interested in the timing of the spike then yet another
simplification is to keep only the voltage dynamics, and then declare
that a spike has occurred whenever the voltage crosses some appropriate threshold,
$v_{thresh}$, which can be related to the sodium activation threshold in real neurons. 
For analytical purposes this turns out to be both convenient and
satisfactory. The basic integrate-and-fire model then is just
\begin{equation}
\begin{gathered}
\dot{v} = \left(I(t) - \frac{(v - \m)}{\tc} \right) \intd{t} 
\\
v(\ts) = v_{thresh} \implies  
\begin{cases}
v(\ts^+) = 0 &  
\end{cases}
\end{gathered}
\label{eq:deterministic_IF}
\end{equation}

In many areas of the brain, due to the large number of connections, 
the external stimulation to a neuron is highly erratic and
unpredictable. When the randomness is mostly in this input, $I$,
one can approximate its effect by adding to the input $I$ a Gaussian white 
noise. In the terminology of this thesis, this is just a term proportional to
a Wiener process increment, $\beta dW_t$. 
Thus the final model of the basic spike-generation mechanism that accounts for
its most important aspects yet retains analytical tractability is the noisy
leaky-integrate-and-fire model:
\begin{equation}
\begin{gathered}
dX_t = \left(\a(t) - \frac{(X_t - \m)}{\tc} \right) \intd{t} + \b \intd{W_t},
\\
X(0) = 0,
\\
X(\ts) = \xth \implies  
\begin{cases}
X(\ts^+) = 0 &  
\end{cases}
\end{gathered}
\label{eq:X_evolution_uo_math_ch}
\end{equation}
That is, $X_t$ follows an Ornstein-Uhlenbeck (OU) process, but upon reaching a pre-determined
threshold, $\xth$, a 'spike' is deemed to have occurred and the process is
'reset' to an initial value, here $0$.

Alternatives to the hard-threshold integrate-and-fire model are so-called
'soft-threshold' models which use a {\sl hazard} function, which is akin to the
intensity of a Poisson Process to determine the spike time. The hazard function
increases as the voltage increases, thus higher voltages imply higher likelihood of
spiking. The hard-threshold model can be seen as a special case of the
soft-threshold model, with the hazard function equal to 0 below the threshold
and infinity above the threshold. We will not address hazard-function-based
spiking models in the rest of the thesis. 

\section{Parameter Estimation for SDEs}
\label{sec:estimation}
Let us rewrite the SDE in \cref{eq:generic_Ito_SDE_Rn} so that the 
functions $B, G$ are explicitly parametrized by some parameter set, $\th$;
\begin{equation}
dX_t = B(X,t;\th) dt + G(X,t;\th) dW,
\label{eq:generic_Ito_SDE_Rn_parameterized}
\end{equation}
For example, in the case of the OU process, \cref{eq:OU_equation_generic},
$B(X,t;\th) = (\m - {X_t})/{\tc}$ and $G(X,t;\th) = \b$, and the
parameter set is $\th = \{\m, \tc, \b\}$. In the standard formulation of the SDE estimation problem,  
one has exact observations $\{x_n\}_{n=0}^N$ at times $\{t_n\}_{n=0}^N$ from a process $X_t$
satisfying \cref{eq:generic_Ito_SDE_Rn_parameterized}, and one seeks to find the
values of the parameter set $\th$.

\subsection{Maximum Likelihood Estimation}
A fundamental method, both
practically and theoretically, for estimating parameters in an SDE is the {\sl Maximum
Likelihoood} (ML) method, which proceeds by seeking those parameters which
maximize the likelihood of the observed data, $\{x_n\}_{n=0}^N$. In particular
let $$L(\{x_n\}; \th) = \Prob[ X_0 = x_0\ldots X_n = x_n\ldots X_N = x_N |
\th]$$ be the joint probability of observing the data $\{x_n\}$ given the
parameter set $\th$, also known as the likelihood. Then the ML method seeks to
maximize $L$.

In the case of independent observations, the
likelihood is just the product of the individual probabilities of each observation. In the case of SDEs, the problem is only slightly more complicated, due to the Markov nature of the stochastic process. In particular,
the likelihood becomes the product of the transition probabilities. Recall that
earlier we defined the transition probability $\f(x,t | y,s)$ in
\cref{eq:transition_prob_defn}. We shall also write this as $\f_\th(x,t|
y,s)$ if we need to be reminded of $\f$'s dependence on the
parameter set, $\th$. The likelihood of the observed $x_n$ then
becomes
\begin{equation}
L(\{x_n\}; \th) = \prod_{n=1}^{N} f_\th(x_n, t_n| x_{n-1}, t_{n-1})
\label{eq:SDE_discrete_likelihood}
\end{equation}
Here, we assume that $x_0$ is fixed, otherwise we would have to add a term
specifying the probability distribution of $X_0$.

In general the transition density for a generic SDE is impossible to find
analytically. There are several ways to approximate it numerically. The most
generic way relies on the numeric solution of the Fokker-Planck PDE in
\cref{eq:generic_FokkerPlanck}, but that is computationally quite expensive and suffers from the
'curse-of-dimensionality' for SDEs of higher dimension. 

In some simple cases, the transition density {\sl can} be calculated. The OU
example described above is one such case, where the transition density is
\begin{align*}
f(x_n, t_n| x_{n-1}, t_{n-1}) &=
 f(x_n, \Delta| x_{n-1}, 0)\\& =
 \frac{1}{ \b \sqrt{\tc 2\pi(1 -  e^{-2 \Delta/\tc}})}
 	\cdot \exp\left(\frac{\left( x - \mu)  - (x_{0} - \mu) \cdot
 	 e^{-\Delta/\tc} \right)^2  } {\t \s^2  (1-e^{-2 \Delta/\tc})}
 	\right) 
\end{align*}
assuming that $\Delta_n = t_n-t_{n-1} = \Delta$ is constant for all $n$.
With this expression, one can form the likelihood and solve analytically for the
maximizers $\{\hat\m , \hat\tc, \hat\b \}$:
\begin{eqnarray} 
\hat{ \mu} &=& 
\frac{  \sum_{n=1}^{N } 
     \left( X_n - e^{-\frac{\Delta} {\hat \tc}} X_{n-1} \right)} 
	 { N( 1-e^{-\frac {\Delta} {\hat \tc}}) }
\\
e^{-\frac {\Delta}{\hat{\tc}} } &=& 
\frac { \sum_{n=1}^{N} 
			( X_n -  \hat \mu)(X_{n-1} -  \hat \mu) }
    {   \sum_{n=1}^{N } \left( X_{n-1} - \hat \mu
    \right)^2 }
\\
\hat\beta^2 &=&  
\frac{ 2  \sum_{n=1}^{N}  \left( X_n - \hat \mu - (X_{n-1} -
\hat \mu) e^{-\frac {\Delta} {\hat \tc}} \right)^2 } 
	  { N (1-e^{-2\frac {\Delta} {\hat \tc}}) \hat \tc}
\end{eqnarray}
The solution for the ML estimates is almost explicit. It requires one numerical
single-dimensional root-finding, which is an easy numerical task. Note that
there is no guarantee that the estimate for $\hat{\beta}$ will be positive in
general.

\subsection{Fortet Equation} 
In the context of \cref{eq:X_evolution_uo_math_ch}, that is for a 1-dimensional
SDE with a boundary above the initial value of the process, $\xth>x_0$, there is
another relation between the hitting-time density, $g$ and the unconditional transition density,
$f$, that is $f$ for which no boundary conditions are applied at $\xth$.

Consider the space integral of $f$
$$ \F(x,t|x_0, t_0) = \int_{\xi<x} f( \xi, t |x_0, t_0)) \intd{\xi},$$
which is just the probability that $X_t \leq x$, conditional on the initial
state, $x_0$. The {\sl Fortet equation} \cite{Fortet1943} states that
\begin{equation}
1 - \F(\xth, t|0, 0) =
\int_0^t g(s) [1-\F (\xth,  t| \xth, s)] \intd{s}.
\label{eq:Fortet}
\end{equation}
The left hand side is simply the probability of exceeding $\xth$ at time
$t$ starting at $0$ at time $0$. This can also be written as the probability
of hitting $\xth$ for the first time at time $s < t$ and then exceeding
$\xth$ at time $t$ starting at $\xth$ at time $s$, integrated over all $s$.

The Fortet equation is particularly appealing to use in parameter estimation
contexts when we have an analytical expression for the unconditional $\F$ as is
the case for example for the Ornstein-Uhlenbeck process. We will see in
\cref{ch:estimate} that \cref{eq:Fortet} can be extended to the case when the
threshold boundary or the dynamics of $X_t$ are not time-homogeneous.

\subsection{Particle Filtering}
In {\sl Bayesian} approaches to parameter estimation, the unknown parameter,
$\th$ is also treated as a Random Variable, $\Th$, with some belief
distribution: 
\begin{equation}
\rho(\th) = \Prob(\Theta = \th)
\label{eq:bayesian_prior} 
\end{equation} which is also called a
prior. Via applications of the Bayes formula, observations from the system are
used to update the belief distribution.

Let us say that the Random Variable $T$ depends on the Random Variable, $\Th$,
which we denote by writing the probability density of $T$ as 
\begin{equation}g(t|\th) =
\Pr(T=t|\Th=\th).
\label{eq:bayesian_observation}
\end{equation}
Suppose we observe $T=t$. Then Bayes' formula
states that 
\begin{equation}
\rho(\th|  t  ) = 
\frac{  \rho(\th) \cdot g(t|\th ; \a) }
	 { \int_\Th  \rho(\th) \cdot  g(t|\th ; \a)  \intd{\th}}
\label{eq:bayesian_formula}
\end{equation}

In practice, exact calculation of $\rho(\th|t)$ would not be possible in our
context, so an approximation approach needs to be made. 

A standard technique is to approximate $\rho$ by a set of weighted particles. To
avoid repetition, we refer to a quick description of how this is done in
\cref{ch:optimal_design}, \cref{sec:intro_to_particle_filtering}, where it is
used.
 
\subsection{Optimal Design}
\label{sec:optimal_design}
There are estimation problems in which the experimenter has some control over
some of the parameters in the model and may choose to set them in order to
facilitate the estimation task. {\sl Optimal design} is the design approach for
statistical experiments that is guided by optimizing some formal measure of the
parameter estimates of the experiments, \cite{Pukelsheim2006}. For example,
perhaps one would like to perform linear regression with a polynomial model and
one has some latitude over the points at which to evaluate the polynomial.
Common criteria when selecting the design, e.g the polynomial points, are
minimizing the determinant or the trace of the covariance matrix of the
parameter estimators. However, the covariance matrix, directly related to the
so-called Fisher Information, often depends on the very parameters one seeks to
estimate.

An alternative to the Fisher Information as an objective for the formal design
of experiments is to use concepts from Information Theory, \cite{MacKay2003}.
A related topic in the Machine Learning literature is called 'Active Learning',
e.g. see \cite{Cohn1996,Settles2010,Seeger2008}. In the third part of the
thesis, \cref{ch:optimal_design}, we will use the {\sl Mutual Information}
criteria as a guideline for choosing the stimulation that best allows parameter
estimation. Thus we now define and discuss the Mutual Information between two
random variables $X, \Th$. For reference, we follow \cite{MacKay2003}.

\begin{defn}Mutual Information:
Given two random variables $T,\Th$ with joint probability density
$p(x,\th)$ and marginal densities $p(x), p(\th)$, the Mutual Information between
$T$ and $\Th$ is given by
\begin{equation}
I(T,\Theta) = \int_\Theta \int_T p(x,\th) \cdot \log \left(
\frac{p(x,\th)}{p(x)p(\th)}\right) \intd{x} \intd{\th}
\label{eq:mutual_info_defn}
\end{equation}
\end{defn}

It is obvious that if $T,\Th$ are independent, then $I(T,\Theta) = 0$. It can
be verified that $I\geq0$ and that it is maximized if $\Theta$ is a function of
$T$; that is, if the entropy of $\Th$ conditional on $T$ is zero. 
The mutual information, $I(T,\Th)$ represents the 'average
reduction in uncertainty about $\Th$ that results from learning the value of
$T$' \cite{MacKay2003}. This statement is formally correct if one takes
 'uncertainty' to mean the entropy of a random
 variable.
 
In order to make use of the Mutual Information in a parameter estimation
context, we again recall the prior and likelihood, $\rho(\th), g(t)$ as defined
in \cref{eq:bayesian_prior,eq:bayesian_observation}.

If we consider $\Th$ as a parameter and $T$ as an observation, it is then
natural to seek an experiment which maximizes the Mutual Information between
$\Th$ and the observation $T$.
 
 The joint distribution can be written as $p(t,\th) =
g(t|\th)\rho(\th)$; the marginal distribution of $\Theta$ is simply the prior, $p(\th) =
\rho(\th)$; and the marginal of $T$ is $p(t) =
\int_\Theta g(t|\th)\rho(\th) \intd{\th}$.
Plugging the three expressions into the definition in
\cref{eq:mutual_info_defn} and cancelling common terms yields
\begin{equation}
I(T,\Th) = \int_\Theta \int_0^{\infty} g(t|\th) 
\log \left( \frac{g(t|\th) }{\rho(\th)\int_\Theta g(t|\th)\rho(\th) \intd{\th}
 } \right)
\intd{t}\intd{\th}.
\label{eq:mutual_info_prior_trajectory}
\end{equation}
\Cref{eq:mutual_info_prior_trajectory}  is used in
\cref{ch:optimal_design}.


\section{Stochastic Optimal Control}
Optimal Control Theory has three main components - a state, $x$, a control,
$\a$, and an objective $J$ which is a functional of $\{x,\a\}$ and which we try
to either minimize or maximize. In {\sl Stochastic} Optimal Control, $x$ follows
a stochastic process, thus the objective, $J$ is most often expressed in terms
of some average or expectation over the random realizations of $x$. The general
theory of Optimal Control, as well as the subset dealing explicitly with random
systems, has relied on two main analytical techniques - {\sl the Maximum
Principle} and {\sl Dynamic Programming}, (a classic reference is
\cite{Fleming1975}). The Maximum Principle uses a variational approach to
characterize the optimal pair, $x_{opt}, \alpha_{opt}$ optimizing $J$, while Dynamic Programming recursively builds up
the optimal solution with a backwards induction from the terminal conditions.
Both techniques have their advantages and disadvantages and we use both in the
thesis. It turns out that there is a close relation between the two approaches,
which is well known in the deterministic finite-dimensional deterministic case,
\cite{Fleming1975,Evansb} and less so in the stochastic case,
\cite{Annunziato2014}.

To set the notation right, we consider the following functional
\begin{equation}
J[\alpha] = \Exp_X \left[ \int_0^\tf L(X_t, \a_t) \intd{t} + M(X_\tf) \right]
\label{eq:generic_objective_functional} 
\end{equation} 
over the realizations of $X_t$ governed by an It\^o SDE as in
\cref{eq:generic_Ito_SDE_Rn}, such that the drift $B$ and/or the diffusion
coefficient, $G$ are parametrized by the control $\a(t)$. Here $L$ is the
running cost function, which depends on the trajectory and the applied control,
while $M$ is the terminal cost function, which just depends on the value of the
trajectory at the terminal time, $t_f$. Here we assume that the terminal time is
a priori known, although this is not necessary in general. 

Given $J$ in \cref{eq:generic_objective_functional}, we then seek $\a$, which
maximizes it (for example): 

\begin{equation}
\a^* = \argmax_{\a \in \Udomain} [ J[\a] ]
\label{eq:generic_optimization_statement}
\end{equation} 

To be mathematically correct, we should specify that the optimization in
\cref{eq:generic_optimization_statement} is done over the space $\Udomain$ of
stochastic processes that are measurable with respect to the filtration
generated by the underlying Wiener process, $W_t$. Further constraints on
$\Udomain$ may be imposed by a specific problem.

The simple-looking \cref{eq:generic_objective_functional} can give rise to
different variations. For one, the final-time $\tf$ may be variable. Or we may
face path or terminal constraints for $X_t$. In the stochastic context such
constraints are only enforceable in a probabilistic sense and they can easily
make the problem much more difficult. However we will not deal with either of
these complications and in the sequel we will assume that $\tf$ is fixed and
that $X_t$ faces no constraints other than that it satisfies its SDE.

Given the objective and the optimization equations,
\cref{eq:generic_objective_functional,eq:generic_optimization_statement}, the
Maximum Principle relies on the forward Kolmogorov (Fokker-Planck)
equation-based definition of the SDE expectation, while Dynamic Programming
relies on the relation between an SDE expectation and the backward Kolmogorov
equation.


\subsection{The Maximum Principle for Transition Probability Densities}
\label{sec:maximum_principle_4_stochastic_control}
The Maximum Principle uses a variational principle to characterize the optimal
control, $\a$ and the optimal {\sl transition } density. Note that the
expectation in the objective, \cref{eq:generic_objective_functional} can be written in terms of the forward
probability density of the state $X_t$:
\begin{align}
J[\a] =&  \Exp_X \left[ \int_0^\tf L(X_s, \a_s) \intd{s} + M(X_\tf) \right]
\notag \\
&=  \int_0^\tf\int_X L(x, \a_s) \cdot f(x,s|x_0,0) \intd{s}\intd{x} 
+ \int_X  M(x)\cdot f(x,\tf|x_0,0)\intd{x}.
\label{eq:generic_objective_functional_in_terms_of_forward_density}
\end{align}
As such, the stochastic problem is reduced to a deterministic optimization
problem but for PDEs, given that $f$ follows the forward PDE in
\cref{eq:generic_FokkerPlanck}. 

Thus, for the purpose of this thesis, the Maximum Principle for Stochastic Optimal
Control is really a Maximum Principle for PDEs. Its variational argument is
similar in spirit to the Euler-Lagrange equations from the Calculus of
Variations; one can think of it as a generalization of the zero-tangent
rule (Fermat's Rule) for finding optima in single-variable calculus. In fact,
like both the Euler-Lagrange equations and Fermat's Rule, the Maximum Principle
provides necessary, but not sufficient conditions for an optimum.

Originally, the Maximum Principle was developed for finite-dimensional
deterministic systems, i.e.\ systems described by ODEs. In that context it is
known as the {\sl Pontryagin} Maximum Principle and for ODEs, the theoretical
results of existence and uniqueness of optimal controls are strongest. 
As is often the case, theoretical results in the infinite-dimensional context,
i.e.\ for PDEs are more difficult to obtain, but the general technique carries
over analogously. Recently, there has been a series of publications on PDE
control of the Fokker-Planck equation, see \cite{Annunziato2010,Annunziato2013,Annunziato2014}.
In particular, \cite{Annunziato2014} discusses the relation between the Dynamic
Programming approach to Stochastic Control and the approach based on PDE
optimization of the Fokker-Planck equation.

The basic idea of the Maximum Principle is that one 'adjoins' the dynamics' PDE
to the objective and introduces a Lagrange multiplier, which in this case is
called {\sl the adjoint state}.

Let us rewrite the governing SDE, \cref{eq:generic_Ito_SDE_Rn}, to explicitly
take into account the control variable $\a(t)$:
\begin{equation}
dX_t = B(X,t; \a) dt + G(X,t) dW.
\label{eq:generic_Ito_SDE_Rn_controlled}
\end{equation} 
Given the problems considered in this thesis, we only
illustrate  a one-dimensional SDE. Its corresponding forward density is
governed by the following Fokker-Planck equation: 
\begin{equation}
\di_t \f= -\di_{x } \left[ B (x,t;\a) \f(x,t) \right] +  
\di^2_{x} \left[ D(x,t) \f(x,t) \right],
\label{eq:generic_FokkerPlanck_controlled}
\end{equation} 
where $D(x,t) = G^2(x,t)/2$. We assume that for all $\a(t)$, both the SDE and the
PDE have unique solutions. 

For notational convenience, we will write
\cref{eq:generic_FokkerPlanck_controlled} as
$$ \dot{\f} = \L_{\a} [\f] $$ where $\L_{\a}$ is the differential operator corresponding to the
right-hand side of the Fokker-Planck equation parametrized by the control $\a$.
For now we will assume that there are no BCs, and the domain of $X$ is all of $\R$.

As we already alluded to, the key concept in Maximum Principle for PDEs is to
adjoin the dynamics, \cref{eq:generic_FokkerPlanck_controlled}, multiplied by
the adjoint state, $p$ to the objective,
\cref{eq:generic_objective_functional_in_terms_of_forward_density}
  
\begin{align*}
J[\a] =& \int_0^\tf\int_X L(x, \a_s) \cdot f(x,s) \intd{s}\intd{x} 
+ \int_X  M(x)\cdot f(x,\tf) \intd{x}
\\ &- \int_0^\tf\int_X p \cdot (\di_t f(x,s)  - \L_{\a} [f(x,s)] )
\intd{s}\intd{x}. 
\end{align*}
Since $f$ satisfies the PDE, we have added a term that equals zero. 
However, what this allows us to do is to 'transfer' the time and space
derivatives from $f$ to $p$. The reason why
that is productive is that we will then be able to form the 'variation'
of $J$ with respect to the control $\a$ and either set it to zero or use this as
a gradient. 

To illustrate the idea, let us perform this 'transfer' explicitly - it is basically an
application of integration-by-parts; in larger dimension this is also commonly
called {\sl Green's identities}.  We have that $$
 \int_0^\tf   p \cdot \di_t f   \intd{t} =
  p\cdot f|_0^\tf - \int_0^\tf   \di_t p \cdot   f  \intd{t}
$$ which 'transfers' the time-derivative to $p$. We also have that $$ \int_X  p \cdot 
\L_{\a} [f ]  \intd{x} = \int_X  ( B  \di_x p + D \di_x^2 p) \cdot  f  \intd{x} =
\int_X   \Lstar_{\a} [p] \cdot f  \intd{x} $$ which transfers' the
space-derivative to $p$. In doing so, we have  naturally introduced 
 the differential operator $\Lstar$, which is the adjoint, in a Banach-space
 sense, to $\L$. Actually, we have already met $\Lstar$ before - it is the
 generator of the SDE in \cref{eq:generic_Ito_SDE_Rn_controlled}.

In the above manipulations, we have assumed that the double integrals have the
same value independent of the order of integration of the space and time
variables  and that $f$ and all its partials go to zero uniformly for $|x|$
large enough. If we had boundary conditions on $f$, those will come up in the
spatial terms.

With the above integration-by-parts done, we can write the objective as
\begin{align}
J[\a] =& \int_0^\tf\int_X L(x_s, \a_s) \cdot f(x,s) \intd{s}\intd{x} 
+ \int_X  M(x)\cdot f(x,\tf) \intd{x} \notag
\\ &- 
\int_X  \left[ p\cdot f|_0^\tf  +
    \int_0^\tf  (\di_t p  + \Lstar_{\a} [p]) \cdot f  \intd{s} \right] \intd{x} 
\label{eq:generic_objective_functional_in_terms_of_forward_density_adjointed}
\end{align}
% (ALEX: check the sign in front of the last integral from 0 to t\_f inside the
% integral of X, as well as in Eq.2.28 below)  - it looks right to me(alex)

Now we assume that we apply a small variation around a given control $\a$: 
\begin{align*}
\a_\e = \a + \e \da
\\
f_\e = f + \e \df
\end{align*}

The variation of the objective, $J$, with respect to the control
variation at the current $\a$ can now be calculated as:
\begin{align}
\frac {dJ}{d\e} \Big|_{\e = 0} &= 
\int_0^\tf\int_X \grad_\a L(x_s, \a_s) \cdot \delta \a \cdot f(x,s) +
L(x_s, \a_s) \cdot \delta f(x,s)
\intd{s}\intd{x} 
\notag \\
&+ 
\int_X  M(x)\cdot \delta f(x,\tf) \intd{x} \notag
\\ &- 
\int_X    p\cdot \delta f(x, \tf)     \intd{x}  \notag \\+& 
    \int_X \int_0^\tf  (\di_t p  + \Lstar_{\a} [p]) \cdot \delta f 
    + \grad_\a B(x,t;\a) \cdot \delta \a \cdot  \di_xp \cdot f 
     \intd{s}
    \intd{x}
\label{eq:J_variation_PDE}
\end{align}
A few notes are required in order to better explain \cref{eq:J_variation_PDE}.
The initial conditions are considered fixed and as such $\delta f |_{t=0} \equiv
0 $, that is the variation in the control does not change $f(x, 0)$. This is
why only the term $p\cdot f|_{t=\tf}$ is retained from \cref{eq:generic_objective_functional_in_terms_of_forward_density_adjointed}.

Heuristically, we would like to infer from \cref{eq:J_variation_PDE} a gradient
with respect to the control, however we note that there are also variations with
respect to $f$ that make it impossible to do so. However, recall that $p$ is
our free variable - the Lagrange multiplier. Thus if we choose $p$
appropriately, we can eliminate $\delta f$ from the expression. Since we
have set up the problem with this in mind, this is now straight forward
to do - we let $p$ evolve (backwards) according to:
\begin{equation}
\begin{cases}
-\di_t p &= \Lstar[p] + L
\\
p(x, \tf) &=  M(x)  
\end{cases}
\label{eq:generic_adjoint_PDE}
\end{equation}

Thus, \cref{eq:J_variation_PDE} simplifies to
\begin{equation}
\frac {dJ}{d\e} \Big|_{\e = 0} =
\int_0^\tf\int_X \left[ \grad_\a L(x_s, \a_s) \cdot f(x,s) +
 \grad_\a B(x,t;\a) \cdot \di_xp \cdot f \right] \cdot \delta \a
     \intd{s}    \intd{x}
\label{eq:J_variation_PDE_simplified}
\end{equation}

% (ALEX: I think it should be \grad_\a in the first term below)
From \cref{eq:J_variation_PDE_simplified}, we can infer that
\begin{equation}
\grad_a J(x,t) = \grad_\a L(x, \a_t) \cdot f(x,t) +
 \grad_\a B(x,t;\a_t) \cdot \di_xp(x,t) \cdot f(x,t)  
\label{eq:J_gradient_wrt_control}
\end{equation}
can be considered as a pointwise gradient of the objective with respect to
changes in the control, given the current control. It is then natural to claim
that setting that equal to zero will give us a necessary condition for an
optimal $\a$.  If setting \cref{eq:J_gradient_wrt_control} to 0 and solving for
$\a$ is possible,  we would then  have an expression of $\a$ in terms of the
state, $f$ and the adjoint $p$, which we could then input in their respective
equations. Of course, this explicit representation of $\a$ would not always be
possible for any running cost function $L$ or drift fields $B$. Even if it were
possible, we  would still have to solve the pair of now-nonlinear
forward/backward equations for $f,p$. It should be clear by now why the Maximum
Principle theory for PDEs faces many practical challenges.

There are two practical approaches to obtain actual numerical results given
\cref{eq:J_gradient_wrt_control}, both of them iterative: 1) gradient
descent and 2) fixed point iteration. 

The fixed point approach is advocated in \cite{Lenhart2007}, for example, but
only for simple pedagogic examples and we will not discuss it here.
Alternatively, we can apply a gradient-based optimization method. Since
\cref{eq:J_gradient_wrt_control} gives us a gradient, we can take the current
control $\a$ and increment it in the direction of $\grad_a J$, if we are
maximizing $J$,  or  $-\grad_a J$, if we are minimizing $J$.

As with all gradient-based optimization, the standard disclaimer about local
minima and optimization initialization applies, since there is no guarantee -  and
 it is usually not the case - that the objective is convex with respect to the
control. 

Our derivations have not been very rigorous. More rigorous arguments can be
found in, e.g.\ \cite{Fattorini1999,Borzi2012}. 
  
As a closing note, we should mention that a Maximum Principle in Stochastic
Control can also be stated directly in terms of the SDE for $X$, in which
case the corresponding 'adjoint' variable satisfies a {\sl backwards} SDE. We
do not explore this approach, but the literature on this topic is also vast, for a
early introductory monograph see \cite{Haussmann1986}.  
 
\subsection{Dynamic Programming for Stochastic Optimal Control}
\label{sec:dynamic_programming}
Dynamic Programming uses backwards recursion to tabulate the optimal control
starting from the terminal time. The basic object in dynamic programming is the
value function, $v$. In order to introduce it, we first extend our definition
for the objective, $J$, to consider starting the state, $X_t$ at later
times, with different initial conditions.

The running cost-to-go corresponding to
\cref{eq:generic_objective_functional} is:
\begin{equation}
J[\a; x, t] = \Exp \left[ \int_t^\tf L(x_s, \a_s) \intd{s} + M(x_\tf) \right] ,
\quad X_t = x
\label{eq:generic_cost_to_go} 
\end{equation}
so that $J[\a; x_0, 0] = J[\a]$ is our original objective from
\cref{eq:generic_objective_functional}.
 
We now introduce the {\sl  value function}, $v$,
defined by: 
$$
v(x,t) = \inf_{\a \in \Udomain} J[\a; x,t];  
$$
it is also called the optimal cost-to-go. 

We immediately note the terminal conditions on $v$:
\begin{equation}
v(x,\tf ) = M(x)
\end{equation}

In other words, if $X_\tf = x$, there is no more time for a control to be applied and no
more running cost to be incurred and we just incur the terminal cost
corresponding to wherever $X$ is now, i.e. $M(x)$. It turns out that the value function, $v$, can be characterized as the solution
to the following non-linear PDE:
\begin{thm}[Hamilton-Jacobi-Bellman (HJB)] 
\label{thm:stochastic_hjb}
\begin{equation}
\begin{cases}
-\di_t v(x,t) &=  \max_{\a(t) \in \Udomain(t)} \big\{ L(x,\a)  +
B(x,t,\a) \cdot \di_x v
\big\} + \frac{G^2(x,t)}{2} \di_x^2v
\\
v(x,\tf) &= M(x)
\end{cases} 
\label{eq:generic_HJB}
\end{equation}
\begin{proof}[Heuristic Derivation adapted from \cite{Evansb}] Suppose we are
at time $t$ and $X_t = x$.Take a time increment $[t, t+h]$ and assume that during that time we apply a
constant control $\a$ and subsequently we apply the optimal control $\a^*$.
The running-cost will then break down as: 
$$
J[\a; x,t] = \Exp \left[ \int_t^{t+h} L(X_s, \a_s) \intd{s}  + v(X_{t+h},
t+h) \right] $$

Since $v(x,t) = \inf J(\a; x,t)$, we must have that:
$$
v(x,t) \leq  \Exp \left[ \int_t^{t+h} L(X_s, \a_s) \intd{s}  + v(X_{t+h},
t+h) \right] $$
or, rearranging, that
\begin{align*}
0 \leq^&  \Exp \left[ \int_t^{t+h} L(X_s, \a_s) \intd{s} \right]  
+ \underbrace{\Exp \left[ v(X_{t+h}, t+h) - v(x,t) \right]}_{\Exp\left[ dv
\right]}
\end{align*}
Now, $\Exp\left[ dv \right]$ can be expressed using Dynkin's Formula, as:
$$
\Exp[dv] = \int_t^{t+h} \di_t v +  B\cdot \di_x v + \frac{G^2 }{2}\cdot \di_x^2
v \intd{s} $$
Plugging that back, we get
$$
0 \leq \int_t^{t+h} L(X,\a) +  \di_tv +  B\cdot \di_x v + \frac{G^2 }{2}\cdot
\di_x^2 v \intd{s} $$
Taking $h \ra 0$ we get 
$$
0 \leq  L(x,\a) +  \di_tv(x,t) +  B\cdot \di_x v(x,t) + \frac{G^2 }{2}\cdot
\di_x^2 v(x,t) $$
and we conjecture that for the actual optimal control, the inequality becomes an
equality:
\begin{equation}
\begin{cases}
0 &=   L(x,\a^*)+ \di_t v(x,t) +
 B(x,t,\a^*)\cdot \di_x v(x,t) + \frac{G^2(x,t)}{2}\cdot \di_x^2 v(x,t)
\\
\a^*  &= \argmax_{\a \in \Udomain(t)}  
\big\{L(x,\a) + B(x,t,\a)\cdot \di_x v \big\}
\end{cases}
\end{equation}
\end{proof}
\end{thm}

It turns out that rigorous proofs of Theorem
\ref{thm:stochastic_hjb} , in particular specifying the exact
mathematical meaning for a solution, $v$, to the HJB PDE are quite complex and beyond the scope
of this text. Some references include \cite{Krylov2008,Fleming2006}. We will
assume that numerically solving \cref{eq:generic_HJB} is sufficient. 

\subsection{Numerical Solutions to PDEs - Finite Difference Methods}
Both the variational approach of the Maximum Principle and the backwards
induction of Dynamic Programming result in having to solve PDEs in order to
obtain the optimal control. For practical purposes, solving these PDEs requires
numerical discretization. In all cases the PDEs are {\sl
parabolic} (see \cite{Press1992}), which in general can be written as 
\begin{equation}
\di_t f(x,t) = \L[f(x,t)]
\label{eq:generic_parabolic_PDE}
\end{equation}
for some differential operator, $\L$. 
In one spatial dimension, a finite difference discretization of
\cref{eq:generic_parabolic_PDE} is to select a set of space- and time- nodes
$\{x_i\}, \{t_k\}$ and approximate $f$ by solving for $f(x_i, t_k)$, given the
initial conditions $f(x_i, t_0)$ and possibly boundary conditions.

A classic technique for parabolic PDEs is the
{\sl Crank-Nicholson} scheme, which time-discretizes
\cref{eq:generic_parabolic_PDE} as
$$
\frac{f(x_i, t_{k+1})- f(x_i, t_k)}{ t_{k+1}-t_k}
=
\frac 12 \left(\L[f(x_i, t_{k+1}  )] + \L[f(x_i, t_k)]
\right)
$$
and then solves for the resulting linear system before stepping
iteratively forward in time. See chapter 19 in \cite{Press1992} for a brief
discussion of the theoretical properties of the Crank-Nicholson method.   

This solution approach requires a finite spatial domain. If the spatial domain
is theoretically infinite, as it may be in some cases, we need to truncate it and
apply some reasonable boundary conditions at the artificial boundary which
approximate the solution of the theoretically infinite space. We will show
details of how this is done in the context of each specific problem
discussed in the thesis.  
 